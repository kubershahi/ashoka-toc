{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "through-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "NUMBER_OF_POSITIONS = 4       # number of states assumed as an upper limit, |Q|\n",
    "\n",
    "# declaring start state\n",
    "START_POSITION = np.zeros(NUMBER_OF_POSITIONS) \n",
    "START_POSITION[0] = 1         # start position is [1 0 0 0]\n",
    "\n",
    "\n",
    "PERCENT_OF_TESTS = 0.1        # percent of data to be used for testing\n",
    "TenzToAdd = 2.0\n",
    "EPS = 0.01                    # error margin to determine successful training\n",
    "NU = 0.7                      # learning rate for tensor\n",
    "NU_ADDER = 0.3                # learning rate for adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "forty-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ba', 1.0], ['bb', 0.0], ['a', 1.0], ['bab', 1.0], ['aa', 0.0], ['baa', 0.0], ['bba', 0.0], ['aa', 0.0], ['aab', 0.0], ['a', 1.0], ['bab', 1.0], ['bbb', 0.0], ['bb', 0.0], ['aa', 0.0], ['b', 1.0], ['b', 1.0], ['ab', 1.0], ['bba', 0.0], ['ba', 1.0], ['ab', 1.0], ['a', 1.0], ['b', 1.0], ['ab', 1.0], ['b', 1.0], ['aaa', 0.0], ['a', 1.0], ['ba', 1.0], ['ab', 1.0], ['b', 1.0], ['b', 1.0], ['b', 1.0], ['ab', 1.0], ['aa', 0.0], ['b', 1.0], ['ab', 1.0], ['a', 1.0], ['a', 1.0], ['baa', 0.0], ['ba', 1.0], ['aa', 0.0], ['bb', 0.0], ['a', 1.0], ['ba', 1.0], ['aba', 1.0], ['abb', 0.0], ['a', 1.0], ['bba', 0.0], ['b', 1.0], ['a', 1.0], ['baa', 0.0], ['baba', 1.0], ['ababababab', 1.0], ['ababababab', 1.0], ['ababa', 1.0], ['abab', 1.0], ['babababab', 1.0], ['abababab', 1.0], ['babab', 1.0], ['babababa', 1.0], ['ababababa', 1.0], ['bababab', 1.0], ['abab', 1.0], ['ababa', 1.0], ['ababababa', 1.0], ['bababababa', 1.0], ['abababa', 1.0], ['bababab', 1.0], ['abab', 1.0], ['ababa', 1.0], ['abab', 1.0], ['abababab', 1.0], ['ababab', 1.0], ['bababababa', 1.0], ['ababa', 1.0], ['babababa', 1.0], ['ababa', 1.0], ['babababa', 1.0], ['bababab', 1.0], ['bababab', 1.0], ['ababababab', 1.0], ['babababab', 1.0], ['babababa', 1.0], ['ababab', 1.0], ['abababab', 1.0], ['abababa', 1.0], ['bababababa', 1.0], ['ababab', 1.0], ['abab', 1.0], ['ababababab', 1.0], ['abababab', 1.0], ['bababa', 1.0], ['ababa', 1.0], ['ababa', 1.0], ['abababa', 1.0], ['abababa', 1.0], ['baba', 1.0], ['baba', 1.0], ['ababababab', 1.0], ['abab', 1.0], ['ababab', 1.0], ['babababab', 1.0], ['ababa', 1.0], ['baba', 1.0], ['ababa', 1.0], ['bababab', 1.0], ['ababababab', 1.0], ['babab', 1.0], ['bababababa', 1.0], ['babababa', 1.0], ['ababab', 1.0], ['ababababab', 1.0], ['babababab', 1.0], ['babab', 1.0], ['ababa', 1.0], ['babab', 1.0], ['ababa', 1.0], ['ababababab', 1.0], ['bababa', 1.0], ['abababab', 1.0], ['ababa', 1.0], ['abababab', 1.0], ['bababab', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['bababa', 1.0], ['babab', 1.0], ['babababab', 1.0], ['babababa', 1.0], ['babab', 1.0], ['abababa', 1.0], ['abab', 1.0], ['baba', 1.0], ['ababab', 1.0], ['abababab', 1.0], ['ababababa', 1.0], ['ababababa', 1.0], ['abab', 1.0], ['baba', 1.0], ['ababababa', 1.0], ['bababab', 1.0], ['bababababa', 1.0], ['ababababab', 1.0], ['ababababab', 1.0], ['ababa', 1.0], ['bababababa', 1.0], ['babab', 1.0], ['ababa', 1.0], ['babab', 1.0], ['bababa', 1.0], ['bababab', 1.0], ['babababa', 1.0], ['bababab', 1.0], ['babab', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['abababab', 1.0], ['ababababa', 1.0], ['babab', 1.0], ['bababababa', 1.0], ['babab', 1.0], ['abab', 1.0], ['abababab', 1.0], ['abab', 1.0], ['abab', 1.0], ['bababa', 1.0], ['abababab', 1.0], ['bababababa', 1.0], ['babababab', 1.0], ['bababab', 1.0], ['babab', 1.0], ['abababab', 1.0], ['bababa', 1.0], ['bababab', 1.0], ['ababab', 1.0], ['abab', 1.0], ['bababababa', 1.0], ['ababababa', 1.0], ['abab', 1.0], ['abababab', 1.0], ['bababab', 1.0], ['baba', 1.0], ['abab', 1.0], ['ababab', 1.0], ['abababab', 1.0], ['babab', 1.0], ['bababababa', 1.0], ['abab', 1.0], ['bababababa', 1.0], ['bababab', 1.0], ['ababab', 1.0], ['babababab', 1.0], ['babababa', 1.0], ['ababab', 1.0], ['ababababa', 1.0], ['babababa', 1.0], ['abababab', 1.0], ['babababab', 1.0], ['bababababa', 1.0], ['bababa', 1.0], ['bababa', 1.0], ['ababab', 1.0], ['bababababa', 1.0], ['abababab', 1.0], ['ababababa', 1.0], ['ababa', 1.0], ['bababababa', 1.0], ['ababababab', 1.0], ['abab', 1.0], ['baba', 1.0], ['ababababab', 1.0], ['baba', 1.0], ['babababab', 1.0], ['bababa', 1.0], ['ababababa', 1.0], ['bababa', 1.0], ['bababab', 1.0], ['baba', 1.0], ['babababab', 1.0], ['bababab', 1.0], ['ababa', 1.0], ['ababababab', 1.0], ['bababab', 1.0], ['bababababa', 1.0], ['bababab', 1.0], ['babab', 1.0], ['ababa', 1.0], ['ababababa', 1.0], ['bababab', 1.0], ['ababa', 1.0], ['bababa', 1.0], ['abab', 1.0], ['ababababab', 1.0], ['babab', 1.0], ['baba', 1.0], ['babababab', 1.0], ['ababab', 1.0], ['ababab', 1.0], ['bababab', 1.0], ['babababa', 1.0], ['abab', 1.0], ['abab', 1.0], ['abab', 1.0], ['bababab', 1.0], ['baba', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['ababababa', 1.0], ['babababa', 1.0], ['abab', 1.0], ['ababab', 1.0], ['ababababab', 1.0], ['abababa', 1.0], ['baba', 1.0], ['bababa', 1.0], ['ababab', 1.0], ['ababa', 1.0], ['ababa', 1.0], ['bababa', 1.0], ['babababa', 1.0], ['abab', 1.0], ['babababa', 1.0], ['abab', 1.0], ['ababababab', 1.0], ['abababab', 1.0], ['bababababa', 1.0], ['ababababab', 1.0], ['abababab', 1.0], ['bababababa', 1.0], ['babababab', 1.0], ['babababab', 1.0], ['babababa', 1.0], ['babababa', 1.0], ['ababababab', 1.0], ['ababab', 1.0], ['ababa', 1.0], ['bababab', 1.0], ['ababa', 1.0], ['babab', 1.0], ['abababab', 1.0], ['bababab', 1.0], ['ababababa', 1.0], ['babababa', 1.0], ['bababababa', 1.0], ['ababab', 1.0], ['babab', 1.0], ['bababab', 1.0], ['babab', 1.0], ['babababa', 1.0], ['ababa', 1.0], ['babababa', 1.0], ['babab', 1.0], ['ababa', 1.0], ['babababa', 1.0], ['baba', 1.0], ['abab', 1.0], ['baba', 1.0], ['abab', 1.0], ['abab', 1.0], ['abab', 1.0], ['abababab', 1.0], ['bababababa', 1.0], ['ababa', 1.0], ['babababa', 1.0], ['ababab', 1.0], ['bababa', 1.0], ['ababababab', 1.0], ['bababababa', 1.0], ['babab', 1.0], ['ababababa', 1.0], ['abab', 1.0], ['ababababa', 1.0], ['ababa', 1.0], ['abababab', 1.0], ['ababab', 1.0], ['abababa', 1.0], ['abab', 1.0], ['ababababab', 1.0], ['bababa', 1.0], ['bababa', 1.0], ['ababababa', 1.0], ['abababab', 1.0], ['ababa', 1.0], ['ababab', 1.0], ['babab', 1.0], ['abababa', 1.0], ['ababababa', 1.0], ['abababab', 1.0], ['ababababab', 1.0], ['ababa', 1.0], ['ababa', 1.0], ['ababababab', 1.0], ['abababa', 1.0], ['ababa', 1.0], ['babababa', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['ababa', 1.0], ['babababab', 1.0], ['abababa', 1.0], ['ababa', 1.0], ['bababababa', 1.0], ['ababababa', 1.0], ['bababababa', 1.0], ['ababab', 1.0], ['babababab', 1.0], ['ababababa', 1.0], ['babababab', 1.0], ['abababa', 1.0], ['abababa', 1.0], ['ababa', 1.0], ['ababababab', 1.0], ['babababa', 1.0], ['baba', 1.0], ['bababababa', 1.0], ['bababa', 1.0], ['babababa', 1.0], ['babab', 1.0], ['bababab', 1.0], ['abababab', 1.0], ['ababababab', 1.0], ['bababababa', 1.0], ['ababababab', 1.0], ['baba', 1.0], ['ababab', 1.0], ['baba', 1.0], ['abababa', 1.0], ['abababab', 1.0], ['bababa', 1.0], ['ababababab', 1.0], ['babababab', 1.0], ['bababa', 1.0], ['ababababab', 1.0], ['ababa', 1.0], ['babab', 1.0], ['abab', 1.0], ['bababa', 1.0], ['baba', 1.0], ['abababa', 1.0], ['bababababa', 1.0], ['abab', 1.0], ['ababab', 1.0], ['babababab', 1.0], ['ababa', 1.0], ['ababab', 1.0], ['babababab', 1.0], ['baba', 1.0], ['ababa', 1.0], ['bababa', 1.0], ['abababa', 1.0], ['bababababa', 1.0], ['ababababab', 1.0], ['ababababa', 1.0], ['bababab', 1.0], ['babab', 1.0], ['ababababab', 1.0], ['abab', 1.0], ['babababa', 1.0], ['bababa', 1.0], ['bababa', 1.0], ['babab', 1.0], ['abab', 1.0], ['baba', 1.0], ['bababab', 1.0], ['ababab', 1.0], ['abababab', 1.0], ['babab', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['babab', 1.0], ['abab', 1.0], ['ababab', 1.0], ['babababa', 1.0], ['abababa', 1.0], ['bababab', 1.0], ['ababababa', 1.0], ['abababab', 1.0], ['bababa', 1.0], ['bababa', 1.0], ['babababa', 1.0], ['bababababa', 1.0], ['bababababa', 1.0], ['babab', 1.0], ['baba', 1.0], ['bababa', 1.0], ['babababa', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['ababababab', 1.0], ['abab', 1.0], ['baba', 1.0], ['abababa', 1.0], ['babab', 1.0], ['bababa', 1.0], ['abab', 1.0], ['ababababa', 1.0], ['abab', 1.0], ['abab', 1.0], ['babababa', 1.0], ['ababab', 1.0], ['babababab', 1.0], ['ababababa', 1.0], ['bababa', 1.0], ['babababa', 1.0], ['bababab', 1.0], ['babab', 1.0], ['ababababab', 1.0], ['bababab', 1.0], ['babababa', 1.0], ['ababababab', 1.0], ['ababab', 1.0], ['ababab', 1.0], ['babababa', 1.0], ['ababababa', 1.0], ['baba', 1.0], ['babab', 1.0], ['ababababab', 1.0], ['baba', 1.0], ['abab', 1.0], ['ababababab', 1.0], ['babababab', 1.0], ['babababab', 1.0], ['ababababab', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['ababab', 1.0], ['babab', 1.0], ['ababababab', 1.0], ['abababa', 1.0], ['bababab', 1.0], ['babababab', 1.0], ['bababab', 1.0], ['ababababab', 1.0], ['bababababa', 1.0], ['bababa', 1.0], ['babab', 1.0], ['babab', 1.0], ['babababab', 1.0], ['bababababa', 1.0], ['ababab', 1.0], ['ababababab', 1.0], ['ababab', 1.0], ['bababababa', 1.0], ['babab', 1.0], ['ababab', 1.0], ['abababab', 1.0], ['abab', 1.0], ['babababab', 1.0], ['bababababa', 1.0], ['babababab', 1.0], ['abababa', 1.0], ['babababa', 1.0], ['abababa', 1.0], ['ababababab', 1.0], ['baba', 1.0], ['babababab', 1.0], ['bababa', 1.0], ['bababa', 1.0], ['ababa', 1.0], ['bababababa', 1.0], ['ababab', 1.0], ['bababababa', 1.0], ['abababab', 1.0], ['babababab', 1.0], ['ababababa', 1.0], ['abab', 1.0], ['abababab', 1.0], ['ababab', 1.0], ['abababab', 1.0], ['babab', 1.0], ['ababab', 1.0], ['ababa', 1.0], ['babab', 1.0], ['bababa', 1.0], ['baba', 1.0], ['ababababab', 1.0], ['babababa', 1.0], ['bababababa', 1.0], ['abababab', 1.0], ['ababababa', 1.0], ['abab', 1.0], ['babababab', 1.0], ['ababababab', 1.0], ['abababa', 1.0], ['babab', 1.0], ['babababa', 1.0], ['ababbbaaa', 0.0], ['babbbabbb', 0.0], ['babaaaa', 0.0], ['bbaaabbaa', 0.0], ['aaaaa', 0.0], ['bbaaa', 0.0], ['abba', 0.0], ['baabbbaab', 0.0], ['aababb', 0.0], ['abaababba', 0.0], ['abaa', 0.0], ['babaabab', 0.0], ['baabb', 0.0], ['abaa', 0.0], ['aaaaaaaab', 0.0], ['abbaabab', 0.0], ['baaababaa', 0.0], ['baaaa', 0.0], ['aabbaa', 0.0], ['abaaabaaab', 0.0], ['aabbaaba', 0.0], ['abbabb', 0.0], ['aaabbbaab', 0.0], ['aabab', 0.0], ['baab', 0.0], ['babaabbaa', 0.0], ['abbaa', 0.0], ['bbaabb', 0.0], ['bbaababb', 0.0], ['bbbaaa', 0.0], ['abba', 0.0], ['bbbabab', 0.0], ['abbbbbabaa', 0.0], ['babbaaabbb', 0.0], ['babbabb', 0.0], ['bbabbbbbab', 0.0], ['aabb', 0.0], ['aaab', 0.0], ['bbbbaaa', 0.0], ['aaaaaaaba', 0.0], ['bbbba', 0.0], ['abaaaba', 0.0], ['bbababaaa', 0.0], ['bbbbbb', 0.0], ['aaabaaa', 0.0], ['aaabab', 0.0], ['bbba', 0.0], ['bbbabbbab', 0.0], ['bbabbba', 0.0], ['bbaaa', 0.0], ['baaab', 0.0], ['aaabbaa', 0.0], ['babbbaa', 0.0], ['babbaabab', 0.0], ['aaaa', 0.0], ['aabbbbb', 0.0], ['baaabbaaa', 0.0], ['ababb', 0.0], ['aabbbbbba', 0.0], ['abaababaa', 0.0], ['abbabbbb', 0.0], ['bbabbababb', 0.0], ['bbaaabbaba', 0.0], ['aababaa', 0.0], ['bbbaaaa', 0.0], ['abbabaaba', 0.0], ['aaabbabbb', 0.0], ['bbaabb', 0.0], ['abbaa', 0.0], ['bbbb', 0.0], ['aaaababb', 0.0], ['bbba', 0.0], ['abbaab', 0.0], ['baaabbaa', 0.0], ['abbbaaaba', 0.0], ['aaaababbab', 0.0], ['aabbbaa', 0.0], ['aababb', 0.0], ['abbaab', 0.0], ['ababbab', 0.0], ['baaaabab', 0.0], ['ababbbbab', 0.0], ['aaba', 0.0], ['bbbba', 0.0], ['baaaaaa', 0.0], ['aaab', 0.0], ['baaaba', 0.0], ['bbabbaab', 0.0], ['abbbbababb', 0.0], ['bbbaa', 0.0], ['bbaaba', 0.0], ['abbababbbb', 0.0], ['abbaaaaa', 0.0], ['abbba', 0.0], ['baabbab', 0.0], ['bbbb', 0.0], ['aaaaaa', 0.0], ['aaba', 0.0], ['abbbbb', 0.0], ['bbaaba', 0.0], ['aaaaaabbb', 0.0], ['aaaabaaa', 0.0], ['ababbba', 0.0], ['bbaaabab', 0.0], ['bbaaaabbaa', 0.0], ['abaababaa', 0.0], ['baabaabb', 0.0], ['baabbbaa', 0.0], ['abaab', 0.0], ['baaaab', 0.0], ['abaabb', 0.0], ['abbbbbaabb', 0.0], ['aaaaabab', 0.0], ['baaabbbab', 0.0], ['bbaabaaabb', 0.0], ['bbbbaba', 0.0], ['abbbbaba', 0.0], ['abaa', 0.0], ['baabaaa', 0.0], ['abbaa', 0.0], ['bbaa', 0.0], ['ababbaba', 0.0], ['aaaaabaaaa', 0.0], ['aaaba', 0.0], ['aaabaaa', 0.0], ['baba', 1.0], ['abaaaaa', 0.0], ['abbbaab', 0.0], ['bbbabbbb', 0.0], ['bbbaa', 0.0], ['aaababaaba', 0.0], ['aababb', 0.0], ['abbbbaaa', 0.0], ['bbaabaaab', 0.0], ['ababbaa', 0.0], ['ababbbba', 0.0], ['aabbbbaba', 0.0], ['abbba', 0.0], ['babababb', 0.0], ['abaaaabbab', 0.0], ['aaabbbbb', 0.0], ['bbbabbbbaa', 0.0], ['bbbbbb', 0.0], ['babbbaaaa', 0.0], ['bbbbbaba', 0.0], ['babbbb', 0.0], ['baababab', 0.0], ['aabbabaaaa', 0.0], ['ababa', 1.0], ['abbbbbbb', 0.0], ['aabbaabab', 0.0], ['babaaba', 0.0], ['babb', 0.0], ['aaabbb', 0.0], ['abbb', 0.0], ['bbaaabba', 0.0], ['bbaaaab', 0.0], ['babaaaaaab', 0.0], ['baaaababa', 0.0], ['baabbaabab', 0.0], ['babaaba', 0.0], ['abbbb', 0.0], ['bbbaabbb', 0.0], ['aaba', 0.0], ['bbab', 0.0], ['abba', 0.0], ['babbabaa', 0.0], ['aaababa', 0.0], ['aabbbab', 0.0], ['aaaaaaaaa', 0.0], ['baabb', 0.0], ['bbbbaba', 0.0], ['bbbbabbb', 0.0], ['babaab', 0.0], ['babaaa', 0.0], ['aaab', 0.0], ['aababaaaab', 0.0], ['abbabbb', 0.0], ['babaa', 0.0], ['aabbababb', 0.0], ['abbbaaaaa', 0.0], ['bbababaabb', 0.0], ['bbaaabaaa', 0.0], ['bbbbb', 0.0], ['babaab', 0.0], ['aabbabaaa', 0.0], ['bbbabba', 0.0], ['abab', 1.0], ['babaabbbb', 0.0], ['ababbb', 0.0], ['abababa', 1.0], ['babbb', 0.0], ['babbb', 0.0], ['aaba', 0.0], ['bbbbbba', 0.0], ['bbabba', 0.0], ['abbbbbaaab', 0.0], ['bbbbaabbba', 0.0], ['aaabaaa', 0.0], ['abbaaaa', 0.0], ['aaaab', 0.0], ['bbabaab', 0.0], ['aaba', 0.0], ['baaabaabab', 0.0], ['bbbbbabb', 0.0], ['aaaabaaab', 0.0], ['baaaabb', 0.0], ['abaaabbab', 0.0], ['bbab', 0.0], ['ababbbaa', 0.0], ['bbbb', 0.0], ['aaaaba', 0.0], ['abbabaabb', 0.0], ['abab', 1.0], ['bbabb', 0.0], ['babbbaab', 0.0], ['aabbab', 0.0], ['aabbbbabaa', 0.0], ['bbbabb', 0.0], ['aaabab', 0.0], ['aababbbab', 0.0], ['abbabaaabb', 0.0], ['bbaaaa', 0.0], ['bbabb', 0.0], ['bbaaaaabba', 0.0], ['bbabb', 0.0], ['aaaabbabba', 0.0], ['bbaabbbbbb', 0.0], ['abbabaa', 0.0], ['aabba', 0.0], ['baabaaaba', 0.0], ['bbbaabbaa', 0.0], ['aaaaa', 0.0], ['abbbbaa', 0.0], ['babbabb', 0.0], ['aaaaabbb', 0.0], ['bbbaaaba', 0.0], ['bbbabb', 0.0], ['bbab', 0.0], ['abab', 1.0], ['bbaaab', 0.0], ['aaabaa', 0.0], ['ababababa', 1.0], ['babababa', 1.0], ['aabbbba', 0.0], ['bbbaaaa', 0.0], ['babababbba', 0.0], ['babbbaa', 0.0], ['aaaaabaa', 0.0], ['bbaa', 0.0], ['bbaaaaa', 0.0], ['bbaaaaa', 0.0], ['abbbaaabba', 0.0], ['bbbaaa', 0.0], ['aaba', 0.0], ['bbba', 0.0], ['baabbb', 0.0], ['abaabbbbba', 0.0], ['ababbbbab', 0.0], ['bbbbab', 0.0], ['baaabb', 0.0], ['aaabab', 0.0], ['aabbbaabba', 0.0], ['babbb', 0.0], ['babbaa', 0.0], ['aabbbaaa', 0.0], ['abbabaabb', 0.0], ['aaabbbbbb', 0.0], ['aabbab', 0.0], ['babbabb', 0.0], ['babbaa', 0.0], ['aababbbaa', 0.0], ['aaabaabaaa', 0.0], ['aaabaa', 0.0], ['aaaaaba', 0.0], ['bbabb', 0.0], ['aabaabaab', 0.0], ['bababaaaaa', 0.0], ['ababa', 1.0], ['aaababbbbb', 0.0], ['abbbbabaa', 0.0], ['babaa', 0.0], ['babbabbba', 0.0], ['bbba', 0.0], ['bbabaaab', 0.0], ['bbabbbabb', 0.0], ['abaabb', 0.0], ['abbab', 0.0], ['aaabaa', 0.0], ['bbbaaa', 0.0], ['aaaba', 0.0], ['baabbb', 0.0], ['aaabbbaaa', 0.0], ['bbabbb', 0.0], ['aababaaab', 0.0], ['abbbabaa', 0.0], ['ababaaaaab', 0.0], ['bbabaab', 0.0], ['bbabba', 0.0], ['abbbbb', 0.0], ['babababbaa', 0.0], ['bbab', 0.0], ['ababbaab', 0.0], ['aaabaaab', 0.0], ['abababaabb', 0.0], ['baabbaab', 0.0], ['babbaba', 0.0], ['bbbbaabb', 0.0], ['abbbabbba', 0.0], ['aabaabbbbb', 0.0], ['aaba', 0.0], ['aabbb', 0.0], ['bbbab', 0.0], ['abbaaaaaba', 0.0], ['baaabbbba', 0.0], ['aababaabb', 0.0], ['aabbaaa', 0.0], ['abbbbbb', 0.0], ['aabbbaaba', 0.0], ['aaabba', 0.0], ['abbbbababa', 0.0], ['aababb', 0.0], ['aababb', 0.0], ['abba', 0.0], ['bababbaa', 0.0], ['abbab', 0.0], ['baabaa', 0.0], ['bbaa', 0.0], ['baabaabba', 0.0], ['aabbaabba', 0.0], ['baabbaabba', 0.0], ['bbaab', 0.0], ['baaaa', 0.0], ['bbbabbaa', 0.0], ['bababb', 0.0], ['baaab', 0.0], ['abbaa', 0.0], ['aababa', 0.0], ['aabbababb', 0.0], ['bbab', 0.0], ['aabbb', 0.0], ['bbbaaabbb', 0.0], ['abaaabaaa', 0.0], ['ababbbaa', 0.0], ['aaabaabb', 0.0], ['baaabaaab', 0.0], ['aabaabab', 0.0], ['bbbaba', 0.0], ['baaaababaa', 0.0], ['aabbb', 0.0], ['bbbbbab', 0.0], ['aaabbbb', 0.0], ['aaabbbab', 0.0], ['babaaba', 0.0], ['aaaabb', 0.0], ['bbaaaabba', 0.0], ['ababbaabb', 0.0], ['bbbbaaaa', 0.0], ['abbaaaabaa', 0.0], ['abbbbbabb', 0.0], ['aaaaaaaaba', 0.0], ['abbbba', 0.0], ['abbaaabb', 0.0], ['aababbb', 0.0], ['abbbabbabb', 0.0], ['aabba', 0.0], ['abba', 0.0], ['bbab', 0.0], ['aaaabaaabb', 0.0], ['bbaab', 0.0], ['aaaaab', 0.0], ['aaba', 0.0], ['abbb', 0.0], ['bbaaaba', 0.0], ['aaabbaaa', 0.0], ['ababab', 1.0], ['baabbaab', 0.0], ['bbbbaababa', 0.0], ['aabbbba', 0.0], ['aabbbbaab', 0.0], ['bbabaab', 0.0], ['aabbabb', 0.0], ['baab', 0.0], ['abbaaabaab', 0.0], ['bababbbb', 0.0], ['bbbbbab', 0.0], ['aaaaaaabbb', 0.0], ['bbaababab', 0.0], ['aaabbba', 0.0], ['bbbaaabbba', 0.0], ['abbbabab', 0.0], ['aabbbabb', 0.0], ['bbba', 0.0], ['abaa', 0.0], ['aabbbbaab', 0.0], ['bbbbab', 0.0], ['aababab', 0.0], ['bbbbbbbbb', 0.0], ['aaababaaa', 0.0], ['bbabaaa', 0.0], ['baabbabb', 0.0], ['bbaa', 0.0], ['bbabbbaa', 0.0], ['aababbaa', 0.0], ['aababaa', 0.0], ['babbbaba', 0.0], ['aaabaabb', 0.0], ['baaa', 0.0], ['aaababa', 0.0], ['aaabb', 0.0], ['babbabb', 0.0], ['baaabaa', 0.0], ['aaaabab', 0.0], ['aaaaba', 0.0], ['babba', 0.0], ['babab', 1.0], ['aabaa', 0.0], ['bababaaaa', 0.0], ['aabba', 0.0], ['aabbabaa', 0.0], ['abaab', 0.0], ['baababbab', 0.0], ['aaababaaba', 0.0], ['abbbab', 0.0], ['baabaa', 0.0], ['aaaaab', 0.0], ['abaaabb', 0.0], ['bbbaaaa', 0.0], ['aabaabba', 0.0], ['baabbbabb', 0.0], ['bbbbb', 0.0], ['bbabbbaa', 0.0], ['bbab', 0.0], ['aabaa', 0.0], ['abaababa', 0.0], ['aaaab', 0.0], ['aaaaaaabb', 0.0], ['baaabaa', 0.0], ['bbbabab', 0.0], ['bbabab', 0.0], ['abba', 0.0], ['aabbaab', 0.0], ['aabaa', 0.0], ['aababaab', 0.0], ['abbbb', 0.0], ['ababbab', 0.0], ['bbaababbba', 0.0], ['bbbba', 0.0], ['bbbbbbabb', 0.0], ['ababa', 1.0], ['bbabbbabb', 0.0], ['bbaaab', 0.0], ['aaba', 0.0], ['aabb', 0.0], ['aaba', 0.0], ['babab', 1.0], ['ababbbb', 0.0], ['baaabb', 0.0], ['aaba', 0.0], ['baabbba', 0.0], ['aaaa', 0.0], ['bbabbaaa', 0.0], ['ababaaaa', 0.0], ['aaababab', 0.0], ['aaaaaabb', 0.0], ['baaa', 0.0], ['bbaba', 0.0], ['aaaababaab', 0.0], ['abbb', 0.0], ['ababbba', 0.0], ['baab', 0.0], ['bbaabbba', 0.0], ['abbaabbbaa', 0.0], ['abbabaaab', 0.0], ['baaabaaaa', 0.0], ['babababaa', 0.0], ['ababaabab', 0.0], ['abaababa', 0.0], ['ababbaba', 0.0], ['abba', 0.0], ['abababbab', 0.0], ['abbaba', 0.0], ['abaababab', 0.0], ['ababbababa', 0.0], ['babababaa', 0.0], ['bbababa', 0.0], ['bababaab', 0.0], ['abaababa', 0.0], ['bababb', 0.0], ['bababaab', 0.0], ['babaabab', 0.0], ['ababaa', 0.0], ['babaabab', 0.0], ['bbaba', 0.0], ['ababaab', 0.0], ['ababababb', 0.0], ['abaababab', 0.0], ['ababbabab', 0.0], ['bababaabab', 0.0], ['babababbab', 0.0], ['babbababab', 0.0], ['aababababa', 0.0], ['ababaabab', 0.0], ['babbababab', 0.0], ['bbabab', 0.0], ['abbaba', 0.0], ['abbab', 0.0], ['babababaab', 0.0], ['baaba', 0.0], ['babba', 0.0], ['ababba', 0.0], ['ababbababa', 0.0], ['babaaba', 0.0], ['baabababa', 0.0], ['baababab', 0.0], ['abaabababa', 0.0], ['abababaa', 0.0], ['bababababb', 0.0], ['aababababa', 0.0], ['babbabab', 0.0], ['abaabab', 0.0], ['baab', 0.0], ['aaba', 0.0], ['abba', 0.0], ['bababbabab', 0.0], ['ababba', 0.0], ['baaba', 0.0], ['babaab', 0.0], ['ababb', 0.0], ['bababbaba', 0.0], ['abababb', 0.0], ['abbabab', 0.0], ['abaab', 0.0], ['bababababb', 0.0], ['abaabab', 0.0], ['babba', 0.0], ['babba', 0.0], ['bababba', 0.0], ['abaabababa', 0.0], ['bbababab', 0.0], ['aabababa', 0.0], ['bbabababa', 0.0], ['abbabab', 0.0], ['abba', 0.0], ['abbaba', 0.0], ['aababa', 0.0], ['aababababa', 0.0], ['baabab', 0.0], ['abababaa', 0.0], ['bababb', 0.0], ['ababba', 0.0], ['bababaaba', 0.0], ['abaabab', 0.0], ['babaaba', 0.0], ['abababba', 0.0], ['bbaba', 0.0], ['babaababab', 0.0], ['abababba', 0.0], ['abbabab', 0.0], ['bbabab', 0.0], ['baabababab', 0.0], ['babb', 0.0], ['ababaa', 0.0], ['ababababb', 0.0], ['baababab', 0.0], ['abaa', 0.0], ['abaa', 0.0], ['abbababab', 0.0], ['bbab', 0.0], ['baaba', 0.0], ['babbaba', 0.0], ['bbab', 0.0], ['babababaa', 0.0], ['babaababab', 0.0], ['ababbab', 0.0], ['aabababa', 0.0], ['ababababb', 0.0], ['bababbaba', 0.0], ['abaaba', 0.0], ['babaa', 0.0], ['abaabab', 0.0], ['babababbab', 0.0], ['babbababab', 0.0], ['ababbab', 0.0], ['bababaab', 0.0], ['bababb', 0.0], ['abbab', 0.0], ['bababababb', 0.0], ['bbababa', 0.0], ['ababaababa', 0.0], ['aabab', 0.0], ['abababaab', 0.0], ['ababba', 0.0], ['babababb', 0.0], ['ababbabab', 0.0], ['abaaba', 0.0], ['babababba', 0.0], ['abababb', 0.0], ['abbab', 0.0], ['abaa', 0.0], ['aababa', 0.0], ['bababababb', 0.0], ['abbabab', 0.0], ['ababbab', 0.0], ['babababba', 0.0], ['abbab', 0.0], ['aaba', 0.0], ['aababa', 0.0], ['abba', 0.0], ['abaababab', 0.0], ['babaababa', 0.0], ['abababbaba', 0.0], ['bbab', 0.0], ['babaaba', 0.0], ['babbab', 0.0], ['babbababa', 0.0], ['abbababa', 0.0], ['aaba', 0.0], ['bababba', 0.0], ['baabab', 0.0], ['babaabab', 0.0], ['baaba', 0.0], ['abba', 0.0], ['babababba', 0.0], ['abba', 0.0], ['bbaba', 0.0], ['abbab', 0.0], ['babbababa', 0.0], ['ababba', 0.0], ['babb', 0.0], ['abaabab', 0.0], ['babaabab', 0.0], ['bababba', 0.0], ['babababbab', 0.0], ['bbababa', 0.0], ['bbababa', 0.0], ['babababaa', 0.0], ['baaba', 0.0], ['aababab', 0.0], ['bbab', 0.0], ['babb', 0.0], ['abababbab', 0.0], ['abbaba', 0.0], ['aabababab', 0.0], ['bbaba', 0.0], ['bababba', 0.0], ['abbababa', 0.0], ['abababbab', 0.0], ['baabab', 0.0], ['bababb', 0.0], ['ababbababa', 0.0], ['abaab', 0.0], ['bbabababab', 0.0], ['baabababab', 0.0], ['ababb', 0.0], ['bbab', 0.0], ['abaababab', 0.0], ['aaba', 0.0], ['baab', 0.0], ['baabab', 0.0], ['ababba', 0.0], ['babba', 0.0], ['aababab', 0.0], ['bbabababa', 0.0], ['bbab', 0.0], ['babaab', 0.0], ['babbab', 0.0], ['bababb', 0.0], ['babaaba', 0.0], ['bababbabab', 0.0], ['bbab', 0.0], ['ababababba', 0.0], ['abababb', 0.0], ['aababab', 0.0], ['baab', 0.0], ['abaabababa', 0.0], ['babbaba', 0.0], ['bababaabab', 0.0], ['abaababa', 0.0], ['bbababa', 0.0], ['bbab', 0.0], ['abaab', 0.0], ['ababaa', 0.0], ['abababbaba', 0.0], ['abbab', 0.0], ['ababaab', 0.0], ['abababaa', 0.0], ['bbab', 0.0], ['abababb', 0.0], ['baab', 0.0], ['baab', 0.0], ['abababbab', 0.0], ['baababab', 0.0], ['abbaba', 0.0], ['abbabababa', 0.0], ['ababbaba', 0.0], ['bababba', 0.0], ['baaba', 0.0], ['aababa', 0.0], ['abaaba', 0.0], ['aabababab', 0.0], ['ababaa', 0.0], ['aabababab', 0.0], ['abba', 0.0], ['abbababa', 0.0], ['babababaab', 0.0], ['ababababba', 0.0], ['bbab', 0.0], ['baabababab', 0.0], ['baabababab', 0.0], ['baaba', 0.0], ['abbababa', 0.0], ['baababa', 0.0], ['babbab', 0.0], ['bbaba', 0.0], ['bbababab', 0.0], ['abbabababa', 0.0], ['ababbabab', 0.0], ['baabababa', 0.0], ['ababababba', 0.0], ['abaa', 0.0], ['babaa', 0.0], ['abbaba', 0.0], ['aabababa', 0.0], ['aaba', 0.0], ['babababbab', 0.0], ['baab', 0.0], ['abbabababa', 0.0], ['bbabab', 0.0], ['ababbababa', 0.0], ['abababaa', 0.0], ['baababab', 0.0], ['bbabababab', 0.0], ['babb', 0.0], ['abababbaba', 0.0], ['baabab', 0.0], ['babaabab', 0.0], ['aababab', 0.0], ['baaba', 0.0], ['aaba', 0.0], ['babaababa', 0.0], ['aaba', 0.0], ['aabab', 0.0], ['bbabababa', 0.0], ['baab', 0.0], ['abaababab', 0.0], ['aababa', 0.0], ['bbababab', 0.0], ['bbababab', 0.0], ['bbabab', 0.0], ['babababba', 0.0], ['bababbaba', 0.0], ['abbabab', 0.0], ['ababababb', 0.0], ['ababbaba', 0.0], ['abba', 0.0], ['babbababab', 0.0], ['babba', 0.0], ['aaba', 0.0], ['babababbab', 0.0], ['babbaba', 0.0], ['ababbab', 0.0], ['abaabab', 0.0], ['babababb', 0.0], ['babbab', 0.0], ['ababb', 0.0], ['babaaba', 0.0], ['babba', 0.0], ['bababaa', 0.0], ['abaababab', 0.0], ['babbaba', 0.0], ['bbaba', 0.0], ['abaababa', 0.0], ['aaba', 0.0], ['abaababab', 0.0], ['abababaa', 0.0], ['abbabababa', 0.0], ['babbababa', 0.0], ['bbabababa', 0.0], ['baab', 0.0], ['bbab', 0.0], ['abaababa', 0.0], ['ababaaba', 0.0], ['bababaab', 0.0], ['babaab', 0.0], ['ababbabab', 0.0], ['babbab', 0.0], ['bbabababab', 0.0], ['ababaa', 0.0], ['babbaba', 0.0], ['babaaba', 0.0], ['bbababa', 0.0], ['ababbabab', 0.0], ['abaa', 0.0], ['abaababa', 0.0], ['bababaab', 0.0], ['bababbabab', 0.0], ['abbababa', 0.0], ['babababbab', 0.0], ['babb', 0.0], ['bababaabab', 0.0], ['ababba', 0.0], ['babbaba', 0.0], ['ababbababa', 0.0], ['aabab', 0.0], ['ababaababa', 0.0], ['abaab', 0.0], ['babbabab', 0.0], ['abaababab', 0.0], ['babababaab', 0.0], ['baababa', 0.0], ['bababb', 0.0], ['bbababab', 0.0], ['bbab', 0.0], ['ababb', 0.0], ['bababaab', 0.0], ['babababbab', 0.0], ['abababaab', 0.0], ['abba', 0.0], ['abaabababa', 0.0], ['bbaba', 0.0], ['bababababb', 0.0], ['abbab', 0.0], ['abababb', 0.0], ['abaabababa', 0.0], ['bbab', 0.0], ['aaba', 0.0], ['babb', 0.0], ['abaababa', 0.0], ['babaa', 0.0], ['babbabab', 0.0], ['aababababa', 0.0], ['ababb', 0.0], ['abbababab', 0.0], ['ababaab', 0.0], ['baaba', 0.0], ['bbabab', 0.0], ['bbabab', 0.0], ['babbaba', 0.0], ['abaab', 0.0], ['bbab', 0.0], ['bbababa', 0.0], ['abaababab', 0.0], ['babababaab', 0.0], ['aaba', 0.0], ['babbab', 0.0], ['ababaa', 0.0], ['ababababba', 0.0], ['aababa', 0.0], ['abbaba', 0.0], ['babaababab', 0.0], ['ababaa', 0.0], ['babb', 0.0], ['abaabab', 0.0], ['babababb', 0.0], ['ababb', 0.0], ['ababaab', 0.0], ['abaa', 0.0], ['bbababab', 0.0], ['aabab', 0.0], ['ababbab', 0.0], ['bababaabab', 0.0], ['abababaab', 0.0], ['ababba', 0.0], ['ababba', 0.0], ['baab', 0.0], ['bbab', 0.0], ['baabababab', 0.0], ['abaabab', 0.0], ['baaba', 0.0], ['abbaba', 0.0], ['babaababa', 0.0], ['abba', 0.0], ['babba', 0.0], ['abababbaba', 0.0], ['bbaba', 0.0], ['abaabababa', 0.0], ['abbaba', 0.0], ['abababba', 0.0], ['abaab', 0.0], ['bababababb', 0.0], ['bababaabab', 0.0], ['abbab', 0.0], ['babbababab', 0.0], ['aababababa', 0.0], ['baaba', 0.0], ['baab', 0.0], ['ababb', 0.0], ['bababababb', 0.0], ['ababaa', 0.0], ['baababa', 0.0], ['bababb', 0.0], ['aaba', 0.0], ['abababaaba', 0.0], ['abababaab', 0.0], ['ababbab', 0.0], ['aababa', 0.0], ['bbaba', 0.0], ['babb', 0.0], ['babaa', 0.0], ['babababaa', 0.0], ['aababa', 0.0], ['babaabab', 0.0], ['babbaba', 0.0], ['bababaaba', 0.0], ['ababbababa', 0.0], ['bababb', 0.0], ['ababb', 0.0], ['aabab', 0.0], ['abaababa', 0.0], ['abbaba', 0.0], ['bababababb', 0.0], ['baaba', 0.0], ['abaabababa', 0.0], ['abaaba', 0.0], ['abbab', 0.0], ['ababaa', 0.0], ['abababba', 0.0], ['bababba', 0.0], ['baab', 0.0], ['abababaab', 0.0], ['abababba', 0.0], ['baab', 0.0], ['ababaababa', 0.0], ['aabababab', 0.0], ['babba', 0.0], ['abababb', 0.0], ['babaa', 0.0], ['abababbab', 0.0], ['ababaabab', 0.0], ['abbab', 0.0], ['ababbab', 0.0], ['abababaa', 0.0], ['ababababaa', 0.0], ['abbabab', 0.0], ['baabab', 0.0], ['babba', 0.0], ['bababbaba', 0.0], ['babababbab', 0.0], ['babababaab', 0.0], ['baababab', 0.0], ['babaab', 0.0], ['babbaba', 0.0], ['bbaba', 0.0], ['baabab', 0.0], ['babaababa', 0.0], ['babb', 0.0], ['baab', 0.0], ['abababaaba', 0.0], ['babaababab', 0.0], ['abaabababa', 0.0], ['ababaaba', 0.0], ['abababbab', 0.0], ['ababaababa', 0.0], ['abababbab', 0.0], ['baabababa', 0.0], ['ababba', 0.0], ['aaba', 0.0], ['abba', 0.0], ['abababbaba', 0.0], ['baaba', 0.0], ['abaa', 0.0], ['bababababb', 0.0], ['aabababab', 0.0], ['babaa', 0.0], ['abaabababa', 0.0], ['bababb', 0.0], ['ababaa', 0.0], ['baab', 0.0], ['bababaabab', 0.0], ['bbabab', 0.0], ['aabab', 0.0], ['bbababa', 0.0], ['babba', 0.0], ['aababababa', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of alphabets set according to the alphabet size, it is known before constructing a DFA\n",
    "DICTIONARY = ['a', 'b'] \n",
    "NUMBER_OF_CHARS = len(DICTIONARY)              # number of alphabets, |A|\n",
    "\n",
    "# getting the dataset of the language\n",
    "f = open('dataset/tl1_dataset.txt', 'r')  # get the dataset for a language\n",
    "dataset = []                              # array to hold the (w,ans) \n",
    "for line in f:\n",
    "    arr = line.split(' ')\n",
    "    ans = 1.0\n",
    "    if arr[1][0] == '0':\n",
    "        ans = 0.0\n",
    "    dataset.append([arr[0], ans])\n",
    "    \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-compatibility",
   "metadata": {},
   "source": [
    "### definition of all the function used in the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bored-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function (ytrue - yloss)^2\n",
    "def cost_function(exp, res):                   \n",
    "    return (res - exp) ** 2\n",
    "\n",
    "# derivative of loss function (ytrue - yloss)\n",
    "def cost_function_derivative(exp, res):       \n",
    "    return res - exp\n",
    "\n",
    "\n",
    "# Match function: takes the nn tensor, current state and the next character and performs the transition\n",
    "# ch is the next character and pos is the current position\n",
    "# Q x S > Q i.e next state = tensor x next character x current state\n",
    "def match(nn, ch, pos):                         \n",
    "    new_pos = np.zeros(NUMBER_OF_POSITIONS)         # create a new array for the next state\n",
    "    for k in range(NUMBER_OF_POSITIONS):            # for each value in the new state\n",
    "        for i in range (NUMBER_OF_CHARS):           # go through next character array\n",
    "            for j in range (NUMBER_OF_POSITIONS):   # go through current staste array\n",
    "                new_pos[k] += nn.tensor[k][i][j] * ch[i] * pos[j] # new state position is tensor x next character x current state\n",
    "    return new_pos                                  # return the new state\n",
    "\n",
    "# derivative of Match function wrt current state: takes the tensor, a gradient and next input character\n",
    "# derivative (2D array)= tensor x next character\n",
    "def match_derivative(nn, dz, ch):\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_POSITIONS]) # dim: |Q| x |Q|\n",
    "    for i in range(NUMBER_OF_POSITIONS):                   # for each column in derivative i.e values in current state [p1 p2 p3 ..]\n",
    "        for k in range(NUMBER_OF_POSITIONS):               # for each row in derivative i.e vlues in new state [h1 h2 h3 ..]\n",
    "            for j in range (NUMBER_OF_CHARS):              # loop through current character  \n",
    "                derivative[k][i] += nn.tensor[k][j][i] * ch[j] # derivative (2D array)= tensor x next character\n",
    "    return np.dot(dz, derivative)                          # dot product of previous gradient with new one\n",
    "\n",
    "# derivative of Match function wrt tensor: take gradient, next input character and current state \n",
    "# derivative(3D array) = next character x current state \n",
    "def match_derivative_tensor(dz, ch, pos):\n",
    "    sample_matrix = np.zeros([NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # create a two 2D array of next character and current state\n",
    "    for i in range(NUMBER_OF_CHARS):\n",
    "        for j in range(NUMBER_OF_POSITIONS):\n",
    "            sample_matrix[i][j] = ch[i] * pos[j]                     # fill the sample matrix vy multiplying next character and current state\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # new derivative as a 3D array\n",
    "    for k in range(NUMBER_OF_POSITIONS): # for each row in first dimension\n",
    "        derivative[k] = dz[k] * sample_matrix      # new derivative is old * sample matrix\n",
    "    return derivative                              # return derivative\n",
    "\n",
    "\n",
    "# normalize function for each value in h_k return h/sum(h)\n",
    "def normalize(h):                    \n",
    "    return h / np.sum(h)\n",
    "\n",
    "# derivative of normalize: takes in current derivative, and input(h) to the output layer\n",
    "def normalize_derivative(dz, h):\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_POSITIONS])\n",
    "    sum = np.sum(h)\n",
    "    for i in range(NUMBER_OF_POSITIONS):      # for each value in the output layer vector \n",
    "        for j in range(NUMBER_OF_POSITIONS):  # for each value in the output layer vector\n",
    "            if(i == j):                       # if position of value wrt to which derivative is being taken is equal, \n",
    "                derivative[i][j] = (sum - h[i]) / (sum ** 2) # derivative is (sum - value)/sum^2\n",
    "            else:\n",
    "                derivative[i][j] = -h[i] / (sum ** 2)        # else -value/sum^2\n",
    "    return np.dot(dz, derivative)             # dot product of previous derivative with new one\n",
    "\n",
    "\n",
    "# applying adder function to output layer, takes the adder and the final state\n",
    "# final output neuron = adder x final state\n",
    "def lastsum(nn, x):\n",
    "    return np.dot(nn.adder, x) # final output neuron = adder x final state\n",
    "\n",
    "# derivative of adder wrt its argument k, takes adder and current derivative\n",
    "def lastsum_derivative(nn, dz):\n",
    "    return np.dot(dz, nn.adder)      # dot product of adder values(k) and current derivative\n",
    "\n",
    "# derivative of adder wrt to k_i's, takes adder, current deriviative and a output layer state\n",
    "def lastsum_derivative_adder(nn, dz, inp):\n",
    "    derivative = np.multiply(inp, nn.adder) # derivative is output layer state x adder \n",
    "    return np.dot(dz, derivative)\n",
    "\n",
    "# function to change a character to vector encodings\n",
    "def char_to_vector(ch):\n",
    "    index = DICTIONARY.index(ch)    # get the index of a character  from dictionary\n",
    "    vec = np.zeros(NUMBER_OF_CHARS) # get a 1D array of len(alphabet) x 1\n",
    "    vec[index] = 1.0                # change the index of character character to 1\n",
    "    return vec                      # return the vector\n",
    "\n",
    "# cut function\n",
    "def cut(x):\n",
    "    if (x > 1.0):\n",
    "        return 1.0\n",
    "    if (x < 0.0):\n",
    "        return 0.0\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-retailer",
   "metadata": {},
   "source": [
    "### RNN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "injured-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network object with two attributes and three methods\n",
    "# two attributes are tensor and adder which are alterable parameters of the network, tensor and adder\n",
    "# three methods are check, train, train_online, get_automaton, and run\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # dim: |Q| x |A| x |Q|, Q is number of states and |A| is alphabet size\n",
    "    tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS])  \n",
    "    adder = np.zeros(NUMBER_OF_POSITIONS)\n",
    "\n",
    "    def __init__(self): # init method that creates tensor and adder attribute and initializes it\n",
    "        \n",
    "        self.tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # dim: to x ch x fr\n",
    "        for fr in range(NUMBER_OF_POSITIONS):  # for each index in fr state\n",
    "            for ch in range(NUMBER_OF_CHARS):  # for each index in a character\n",
    "                z = np.random.rand(NUMBER_OF_POSITIONS) # get a random value intialized array of lenght |Q|\n",
    "                z = normalize(z)                        # normalize it \n",
    "                for to in range(NUMBER_OF_POSITIONS):   # for each index in to state\n",
    "                    self.tensor[to][ch][fr] = z[to]     # initialize the given index of tensor\n",
    "                    \n",
    "        self.adder = np.zeros(NUMBER_OF_POSITIONS)      # dim: |Q| x 1\n",
    "        for to in range(NUMBER_OF_POSITIONS):           # for each index in the adder array\n",
    "            # initialize each index with a value corresponding to its index, value = (index + 1)/|Q|\n",
    "            self.adder[to] = 1.0 * (to + 1) / (NUMBER_OF_POSITIONS) \n",
    "\n",
    "\n",
    "    def check(self, word): # method to check whether a word belongs to the language or not\n",
    "        curr_pos = START_POSITION                       # start from start position \n",
    "        for k in range(len(word)):                      # for each character in the word\n",
    "            curr_word = char_to_vector(word[k])         # convert it to vector form \n",
    "            curr_pos = match(self, curr_word, curr_pos) # make a transition \n",
    "            curr_pos = normalize(curr_pos)              # normalize the next state\n",
    "        output = lastsum(self, curr_pos)                # then apply an adder function to get a final output\n",
    "        return output\n",
    "\n",
    "    def train_online(self, dataset): # method to online train the network i.e weight are adjusted after every word is read\n",
    "        average_error = 1.0          # initializing and average error\n",
    "        epoch_number = 0             # epoch number\n",
    "        n = len(dataset)             # number of words in the datset\n",
    "        tests_size = int(PERCENT_OF_TESTS * n) # number of words allocated for testing the dataset\n",
    "        while (average_error > EPS):           # while the error is greater than specified epsilon  \n",
    "            random.shuffle(dataset)            # shuffle the datset for better training\n",
    "            cases_left = len(dataset)          # varaible to track the number of words lefts to train on\n",
    "            epoch_number += 1                  # increment the epch by 1\n",
    "            print ( 'Epoch #' + str(epoch_number))  # print epoch number\n",
    "            while(cases_left > tests_size):         # for a epoch train on the whole dataset \n",
    "                self.train(dataset[cases_left - 1][0], dataset[cases_left - 1][1]) # training is done from the last word\n",
    "                cases_left -= 1\n",
    "            average_error = 0.0                    \n",
    "            for i in range(cases_left):         # for each word in the dataset \n",
    "                average_error += cost_function(dataset[i][1], self.check(dataset[i][0])) # predict and calculate the error\n",
    "            average_error /= cases_left         # normalize the average error\n",
    "            print (\"Average error: \" + str(average_error)) # print the average error\n",
    "\n",
    "    def train(self, word, exp):\n",
    "        cut_v = np.vectorize(cut)\n",
    "        word_length = len(word)\n",
    "        positions = np.zeros([word_length + 1, NUMBER_OF_POSITIONS])\n",
    "        before_normalize = np.zeros([word_length, NUMBER_OF_POSITIONS])\n",
    "        d_tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS])\n",
    "        d_adder = np.zeros(NUMBER_OF_POSITIONS)\n",
    "        positions[0] = START_POSITION\n",
    "\n",
    "        for k in range(word_length):\n",
    "            curr_word = char_to_vector(word[k])\n",
    "            before_normalize[k] = match(self, curr_word, positions[k])\n",
    "            positions[k + 1] = normalize(before_normalize[k])\n",
    "        answer = lastsum(self, positions[-1])\n",
    "        error = cost_function(exp, answer)\n",
    "\n",
    "\n",
    "        gradient = cost_function_derivative(exp, answer)\n",
    "        d_adder += lastsum_derivative_adder(self, gradient, positions[-1])\n",
    "        gradient = lastsum_derivative(self, gradient)\n",
    "        first_gradient = sum(abs(gradient)) * TenzToAdd\n",
    "        for k in range(word_length - 1, -1, -1):\n",
    "            curr_grad = sum(abs(gradient))\n",
    "            if (curr_grad < 0.001):\n",
    "                koef = 1.0\n",
    "            else:\n",
    "                koef = first_gradient / sum(abs(gradient))\n",
    "            gradient *= koef\n",
    "            curr_word = char_to_vector(word[k])\n",
    "            gradient = normalize_derivative(gradient, before_normalize[k])\n",
    "            d_tensor += match_derivative_tensor(gradient, curr_word, positions[k])\n",
    "            gradient = match_derivative(self, gradient, curr_word)\n",
    "\n",
    "        d_tensor /= word_length\n",
    "        self.tensor = cut_v(self.tensor - NU * d_tensor)\n",
    "        self.adder = cut_v(self.adder - NU * NU_ADDER * d_adder)\n",
    "        return error\n",
    "\n",
    "    def get_automaton(self): # method to get the DFA, traverse through 3D array tensor\n",
    "        \n",
    "        for i in range(NUMBER_OF_POSITIONS): # for each index of a state, last index of 3D array, from state\n",
    "            for j in range(NUMBER_OF_CHARS): # for each index of a character, middle index of 3D array, character\n",
    "                max_ind = 0                  # set max_ind as 0 initially\n",
    "                for k in range(1, NUMBER_OF_POSITIONS): # for each index of a state, first index of 3D array, to state\n",
    "                    if (nn.tensor[k][j][i] > nn.tensor[max_ind][j][i]): # find the index with the maximum value \n",
    "                        max_ind = k                                     # set it to k\n",
    "                print(str(i) + \"--\" + str(DICTIONARY[j]) + '-->' + str(max_ind)) # then from(i) state upon consuming(j)\n",
    "                                                     # has maximum probability of going to to(k) state\n",
    "        \n",
    "        for k in range(NUMBER_OF_POSITIONS):  # for each index of a state in adder vector\n",
    "            if (nn.adder[k] > 0.5):           # if the value of the a particular index is greater than 0.5\n",
    "                print(str(k) + \" is terminal\") # then it has more than 50% chance to be terminal\n",
    "    \n",
    "    \n",
    "    def run(self, word): # method to run a word on the trained dfa and check whether the word gets accepted or rejected\n",
    "        output = nn.check(word)        # call check method\n",
    "        if (output > float(0.6)):      # if the probability is greater than 60 %\n",
    "            ans = 'Accepted'           # then accept the word\n",
    "        else: \n",
    "            ans = 'Rejected'           # or reject the word\n",
    "        return output, ans             # return both output and ans\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "compliant-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "Average error: 8.4795933554787e-08\n",
      "0--a-->1\n",
      "0--b-->2\n",
      "1--a-->3\n",
      "1--b-->2\n",
      "2--a-->1\n",
      "2--b-->3\n",
      "3--a-->3\n",
      "3--b-->3\n",
      "1 is terminal\n",
      "2 is terminal\n"
     ]
    }
   ],
   "source": [
    "# training the NN and getting the DFA\n",
    "\n",
    "nn = NeuralNetwork();\n",
    "nn.train_online(dataset)\n",
    "nn.get_automaton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "governing-telescope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected\n"
     ]
    }
   ],
   "source": [
    "output, ans = nn.run(\"abaaba\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "compliant-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
