{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "NUMBER_OF_POSITIONS = 2       # number of states assumed as an upper limit, |Q|\n",
    "\n",
    "# declaring start state\n",
    "START_POSITION = np.zeros(NUMBER_OF_POSITIONS) \n",
    "START_POSITION[0] = 1         # start position is [1 0 0 0]\n",
    "\n",
    "\n",
    "PERCENT_OF_TESTS = 0.1        # percent of data to be used for testing\n",
    "TenzToAdd = 2.0\n",
    "EPS = 0.01                    # error margin to determine successful training\n",
    "NU = 0.7                      # learning rate for tensor\n",
    "NU_ADDER = 0.3                # learning rate for adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "forty-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bbbbaabbabaaaaabb', 0.0], ['ba', 1.0], ['bbbabba', 0.0], ['ab', 1.0], ['abaabaaabbaa', 1.0], ['babaaaa', 0.0], ['aabbaaaabbbabaabbaaabaa', 0.0], ['ababbbbabaabbaaabba', 0.0], ['baa', 0.0], ['abaabbbbabbbbaaabbabaaabaaa', 0.0], ['aaaaba', 1.0], ['bbbbba', 1.0], ['abbbbabababaaabababbabbba', 0.0], ['bbabab', 1.0], ['aabbbabbabaaaabbbaabbbaaba', 1.0], ['babaa', 0.0], ['aaababb', 0.0], ['bbbbb', 0.0], ['ababbbaabaaaba', 1.0], ['abbbb', 0.0], ['abbbabbabbabba', 1.0], ['bababbbbbbabbaaab', 0.0], ['bb', 1.0], ['baab', 1.0], ['bbbbbbbaaaaba', 0.0], ['bb', 1.0], ['bbbb', 1.0], ['aaababb', 0.0], ['baababaabbaaabb', 0.0], ['bbb', 0.0], ['ab', 1.0], ['a', 0.0], ['baaabbab', 1.0], ['bbbbbba', 0.0], ['abbbbbbbbabbbaaabb', 1.0], ['ababaabbaababababbabb', 0.0], ['bb', 1.0], ['bbabaaaaababa', 0.0], ['aabbababbbaaababbaaaab', 1.0], ['bbabbaaa', 1.0], ['abbbbaaaaabbabbaaabb', 1.0], ['aabba', 0.0], ['aaaab', 0.0], ['baabaaba', 1.0], ['bbbababbaa', 1.0], ['abab', 1.0], ['bba', 0.0], ['bbbbbbbaababaaabbbaabbbabba', 0.0], ['aababbaaabbbbabbbaabbabaabb', 0.0], ['b', 0.0], ['bbaaa', 0.0], ['aababbbaaabba', 0.0], ['bbabb', 0.0], ['baaba', 0.0], ['babbaa', 1.0], ['bbbaa', 0.0], ['ababababbaababbbbbaaabaaabbb', 1.0], ['abbbab', 1.0], ['abbabbaabababbababbbbaa', 0.0], ['aaa', 0.0], ['a', 0.0], ['bbba', 1.0], ['abbab', 0.0], ['bbbabbaabbab', 1.0], ['bbbbaabababaaab', 0.0], ['babaabaaaaabababaabbbbbbbbb', 0.0], ['abbaaabbaababbbbaaa', 0.0], ['babababbabbaaabaabbbaaaa', 1.0], ['abbbbb', 1.0], ['bbaabb', 1.0], ['bb', 1.0], ['a', 0.0], ['bab', 0.0], ['abba', 1.0], ['bb', 1.0], ['babbaaabbaa', 0.0], ['baabaaaaaaabbaaaabaabaabba', 1.0], ['aababbba', 1.0], ['bbaaaab', 0.0], ['aaaabbbabaaaabaababbaaa', 0.0], ['aab', 0.0], ['bba', 0.0], ['ababbba', 0.0], ['aabaaaabbba', 0.0], ['babaabbababbaabb', 1.0], ['aababa', 1.0], ['aab', 0.0], ['baab', 1.0], ['bbbb', 1.0], ['bbbbb', 0.0], ['bbbaabbabaabbabbaababbba', 1.0], ['aabbaaa', 0.0], ['abbbbababbbbbbbababbabbbaab', 0.0], ['abababa', 0.0], ['aabaaababaaabababbbbaaabb', 0.0], ['babb', 1.0], ['a', 0.0], ['abaaabb', 0.0], ['babb', 1.0], ['bbabaaa', 0.0], ['bbbbbbabaababababaaaabbabbaaa', 0.0], ['a', 0.0], ['baa', 0.0], ['babbababaabbabbabbbabbbbb', 0.0], ['bbbaaabbaaabbbabaabbbababbaba', 0.0], ['aababbbbbaabaab', 0.0], ['bb', 1.0], ['abbabb', 1.0], ['bbbba', 0.0], ['abaaabba', 1.0], ['ab', 1.0], ['baa', 0.0], ['bbababbbabbbbbaabbbab', 0.0], ['abaaaaabbbb', 0.0], ['abbabaabbaaababbbbbbbb', 1.0], ['baaabbbaaabaabbaaabaaaaabbaab', 0.0], ['aaaabbaaabbbabbabaabbabba', 0.0], ['aabaaaaaaaaabaaabbbabab', 0.0], ['baabbbaabbbaabaaabaa', 1.0], ['bbabababaabbbabbbbbbabaaabab', 1.0], ['baa', 0.0], ['baabbabababbaaabbbabba', 1.0], ['bb', 1.0], ['bbbaaaaabaa', 0.0], ['a', 0.0], ['aabba', 0.0], ['aaababbaaabb', 1.0], ['bbaabaaa', 1.0], ['bbabaababaaaaaaabbaaabbbaabb', 1.0], ['aaba', 1.0], ['bababbaabbbab', 0.0], ['abbbaaa', 0.0], ['aba', 0.0], ['aaababbb', 1.0], ['baaabababbbababa', 1.0], ['aaaabba', 0.0], ['ab', 1.0], ['baa', 0.0], ['bbbb', 1.0], ['ba', 1.0], ['abbbbabbbbabaabbaaa', 0.0], ['aabbbbbbbbbaabbabbaa', 1.0], ['abbbbaa', 0.0], ['ababbaaba', 0.0], ['aaaa', 1.0], ['bb', 1.0], ['b', 0.0], ['a', 0.0], ['ba', 1.0], ['bbbb', 1.0], ['baaaaabbbbbabababaabaaabbbaab', 0.0], ['b', 0.0], ['bbbbbaa', 0.0], ['baaaba', 1.0], ['abbb', 1.0], ['bbabbaabbab', 0.0], ['baa', 0.0], ['baabbab', 0.0], ['bbaa', 1.0], ['bbbbbaba', 1.0], ['baabbbaab', 0.0], ['ba', 1.0], ['babbbbbaabbbbbabbaa', 0.0], ['bbbbabababbbababbabbabaabbbab', 0.0], ['abbabbbabbbbbaab', 1.0], ['aaab', 1.0], ['abbba', 0.0], ['a', 0.0], ['babaaababaababbaababbbaaa', 0.0], ['aaa', 0.0], ['bbbbabbbaaaa', 1.0], ['baaabaaba', 0.0], ['aabbaaababbbbabab', 0.0], ['abbbbbaabaabbbabaaa', 0.0], ['bbaaaa', 1.0], ['bbbabbab', 1.0], ['aababb', 1.0], ['abbb', 1.0], ['bbaba', 0.0], ['abbba', 0.0], ['babbaabbbbaaaaaabaabbbabbb', 1.0], ['baaaaaaabba', 0.0], ['abb', 0.0], ['babbbbabaaba', 1.0], ['aabaabaababbabbbaabb', 1.0], ['baaabababbbbbbbabbba', 1.0], ['aaaaa', 0.0], ['aab', 0.0], ['baaaa', 0.0], ['bbbabaab', 1.0], ['bbabbbabaaaabbbbbbaaabbaabb', 0.0], ['bbaabaaabbbabbbbaabbbaba', 1.0], ['aabb', 1.0], ['baa', 0.0], ['bbabbaa', 0.0], ['abbabbbaaabbabbbbaabbbb', 0.0], ['babbbbabaaaabaaaaab', 0.0], ['bbbaaa', 1.0], ['abaaaab', 0.0], ['abbaaabb', 1.0], ['aabbbabbbbbabbbabaaabaaaababb', 0.0], ['abb', 0.0], ['aabbaaaaaabaaabbabb', 0.0], ['abbab', 0.0], ['bbbaabaaaabbaabbbbabaaaab', 0.0], ['aabbabb', 0.0], ['aaabba', 1.0], ['bb', 1.0], ['abaaba', 1.0], ['abbabba', 0.0], ['aaabba', 1.0], ['abaabbbaba', 1.0], ['aabbabbaabbaaababa', 1.0], ['ba', 1.0], ['aab', 0.0], ['aaabbabaabaaabb', 0.0], ['bbaa', 1.0], ['a', 0.0], ['baa', 0.0], ['aaababaababaabaabb', 1.0], ['babb', 1.0], ['aabab', 0.0], ['abaab', 0.0], ['aabbbbbaabaabaabb', 0.0], ['bbbaaababbabaaaaa', 0.0], ['abbba', 0.0], ['babbbbbabbbaaaabb', 0.0], ['bbab', 1.0], ['bbaabab', 0.0], ['baaaabba', 1.0], ['aaaaa', 0.0], ['baaaabaaabaabbbaabaaaabab', 0.0], ['aabba', 0.0], ['bbabbabbaaba', 1.0], ['bbbbb', 0.0], ['bbb', 0.0], ['a', 0.0], ['abaaaaaabaaabbbbbbaabbabb', 0.0], ['bb', 1.0], ['bbab', 1.0], ['ab', 1.0], ['bb', 1.0], ['aaaabaa', 0.0], ['babaababbabbb', 0.0], ['aa', 1.0], ['bbbab', 0.0], ['aababbbaabbaaabbbbaababbabba', 1.0], ['b', 0.0], ['abbaaaaabababbbbabbaaba', 0.0], ['aabbbabababbb', 0.0], ['abbbaaaa', 1.0], ['aa', 1.0], ['bba', 0.0], ['aabaaa', 1.0], ['bbbbbbb', 0.0], ['bbbbbaaaab', 1.0], ['b', 0.0], ['abaaaabaabbaaaaaaabbbaaabaa', 0.0], ['a', 0.0], ['baabaaaaaaabaaa', 0.0], ['aab', 0.0], ['abbbbaaabbabaabbbbababbaabab', 1.0], ['abbbb', 0.0], ['bb', 1.0], ['ababbbbbbabaaabaaab', 0.0], ['abaababaaabbabbbbabaaaabaabb', 1.0], ['aaaaaaabaa', 1.0], ['abbb', 1.0], ['bbaaaaabaabaabbbbabaa', 0.0], ['abaababa', 1.0], ['aabaaaba', 1.0], ['abbaa', 0.0], ['aaa', 0.0], ['aabbaabbbbaababbbaaabbbbabba', 1.0], ['aaaaabba', 1.0], ['bbaaab', 1.0], ['bbaaa', 0.0], ['abbba', 0.0], ['bababba', 0.0], ['bbaaab', 1.0], ['abaaabaa', 1.0], ['aabaababaabaaababb', 1.0], ['baabbab', 0.0], ['bbbab', 0.0], ['aabbaaaab', 0.0], ['babbbbbaa', 0.0], ['aaaa', 1.0], ['baab', 1.0], ['aaaa', 1.0], ['babaaaab', 1.0], ['babababbbab', 0.0], ['abbab', 0.0], ['a', 0.0], ['abaaaab', 0.0], ['aaabbbbb', 1.0], ['aabbb', 0.0], ['bbb', 0.0], ['abaaaab', 0.0], ['bbbabbbabaaabbbbaaabaaaba', 0.0], ['abbba', 0.0], ['ababaab', 0.0], ['abb', 0.0], ['ba', 1.0], ['babbbbbbbbabaaababbaabbaa', 0.0], ['bbb', 0.0], ['aaaaaaba', 1.0], ['baaaaaa', 0.0], ['babbaabaabaaaaababaaabbbbaaab', 0.0], ['bbbb', 1.0], ['bbabab', 1.0], ['bbbababaabb', 0.0], ['abb', 0.0], ['babbbabbababaaaab', 0.0], ['bbbabbbbbaabbbbbababb', 0.0], ['bbabbbbbbbaaaaabbbbaaba', 0.0], ['bbbbaba', 0.0], ['bbbbaabbb', 0.0], ['bbbbabbbbbbbbabb', 1.0], ['ba', 1.0], ['ababbaa', 0.0], ['baba', 1.0], ['aaaabaa', 0.0], ['abbababbbababbb', 0.0], ['abbabbbbabbbabaaabbbbaaabbbaa', 0.0], ['abbbaabb', 1.0], ['bbaabbab', 1.0], ['bbbbbbabbabbabababbaabbabb', 1.0], ['aaaabaabaaaabaaaababbababa', 1.0], ['abaaaaba', 1.0], ['aabba', 0.0], ['b', 0.0], ['aaaaaabaaaaaababb', 0.0], ['abababbb', 1.0], ['ba', 1.0], ['baab', 1.0], ['aba', 0.0], ['bbbb', 1.0], ['aaaaaababbbabbaababbbbbab', 0.0], ['bbaabbbbbaaaaaaabbbbaabb', 1.0], ['aabbbbabbbabbaabaaaabaaaaaaa', 1.0], ['aaabbbbabbaabbbbbbbaab', 1.0], ['babb', 1.0], ['abbabbaabbabbbbbabaabb', 1.0], ['a', 0.0], ['abbbababbbaaabb', 0.0], ['a', 0.0], ['aaabbaa', 0.0], ['bbbabbababbabbabaaabab', 1.0], ['aaab', 1.0], ['bababaaabbabbabbaabbbbaaa', 0.0], ['ababaabbaabbaaaaaab', 0.0], ['bbbababbabbabbbaaabab', 0.0], ['bbba', 1.0], ['b', 0.0], ['aaabaaababbbbbbbbbaababbaab', 0.0], ['a', 0.0], ['aaababbab', 0.0], ['bbbbabab', 1.0], ['bababbaba', 0.0], ['b', 0.0], ['baabbab', 0.0], ['bbabb', 0.0], ['baaaaba', 0.0], ['bbabbbbbaba', 0.0], ['aabbaabbaabbbabaabbaa', 0.0], ['abbabbbbaaabbabbababaab', 0.0], ['bba', 0.0], ['aba', 0.0], ['aabaaabbbaababaabaaba', 0.0], ['babbaaababaa', 1.0], ['b', 0.0], ['aabbbaa', 0.0], ['babbabaaabbabaabb', 0.0], ['abababaaabaababbaab', 0.0], ['aba', 0.0], ['aaabbbaababaabbbbaabbaaaa', 0.0], ['aabbbbbababbaaabaabaabbababab', 0.0], ['bababbbabaababbabbaaabaa', 1.0], ['babbabaaaaaaaabbbabbaabbababa', 0.0], ['bbababbaaaaabbbbbbbbab', 1.0], ['bbabaaababbabaababbbbabbbaab', 1.0], ['aaabbbab', 1.0], ['bbbbbaaaaaabbbaaabaabaaab', 0.0], ['aaabaaaaaaaaabb', 0.0], ['baa', 0.0], ['aa', 1.0], ['baaaabbbbaaaaabbabbab', 0.0], ['abbbbba', 0.0], ['abaabba', 0.0], ['bbbbaab', 0.0], ['abbbbbbabaaaaaababbbaabbabaa', 1.0], ['bababaaaabaab', 0.0], ['aabbbabbaaabaaa', 0.0], ['bb', 1.0], ['a', 0.0], ['baaba', 0.0], ['bbbb', 1.0], ['bbabbbbabaaabbabbbba', 1.0], ['ba', 1.0], ['aababab', 0.0], ['bbbaaaa', 0.0], ['abbbbabaa', 0.0], ['baababaabbaaababbaaabbbb', 1.0], ['aabbbbaabbaaaaab', 1.0], ['ababbbabbaabbbbaaaaaaab', 0.0], ['bbaab', 0.0], ['bbbbbabbbbaabbaabbbbaabbbaaaa', 0.0], ['abbbababbbababaaaaaababbbbab', 1.0], ['a', 0.0], ['bbab', 1.0], ['bbbaa', 0.0], ['abbbabaaaaaba', 0.0], ['abbabaaabababb', 1.0], ['baabba', 1.0], ['aba', 0.0], ['abbbaaababaaaabababaab', 1.0], ['baaaaabbbbaababbabaaabbbbabab', 0.0], ['aabb', 1.0], ['baab', 1.0], ['bbbaabaa', 1.0], ['aa', 1.0], ['baabbba', 0.0], ['aab', 0.0], ['babaaabaababa', 0.0], ['baaab', 0.0], ['aabbbababaaabaababbbbbaaaaa', 0.0], ['aabba', 0.0], ['b', 0.0], ['bbbabbabbbaaaababaaabab', 0.0], ['a', 0.0], ['bbab', 1.0], ['bbabbb', 1.0], ['aaabbbbba', 0.0], ['ababbba', 0.0], ['bbbaabaababaabbbbbbbaba', 0.0], ['aa', 1.0], ['babbbaabbaabababbbaabaaba', 0.0], ['abbaaaabaa', 1.0], ['aabaabaa', 1.0], ['babbbbb', 0.0], ['babaaaaabbabababaabbabba', 1.0], ['aba', 0.0], ['b', 0.0], ['aabbaabbbabaaab', 0.0], ['baabba', 1.0], ['bbb', 0.0], ['abaababbabaaabbaaaa', 0.0], ['aabaaaabbbaabba', 0.0], ['aabababbaabbbabbbbabaaabaabb', 1.0], ['baabbbabbbbaaabbbbababbbaab', 0.0], ['aabbbaa', 0.0], ['aba', 0.0], ['abababaabaaaababab', 1.0], ['babaabab', 1.0], ['aa', 1.0], ['aaababbbbbbbbbaaaaababbaaaa', 0.0], ['bbabbbabbbaabbbab', 0.0], ['aaa', 0.0], ['bbbaaababbabbbaaababaaaaaba', 0.0], ['aaabaaaaaaab', 1.0], ['aaab', 1.0], ['ababbaababaaabbabbabbbaa', 1.0], ['bbabaa', 1.0], ['abaa', 1.0], ['ababba', 1.0], ['abbabbbabaaaaaa', 0.0], ['bbbbbbaabbbb', 1.0], ['aabbaaa', 0.0], ['bbaabaaaabb', 0.0], ['bbaab', 0.0], ['aaabb', 0.0], ['abbabbbbaaabaababbb', 0.0], ['baa', 0.0], ['aaaba', 0.0], ['aabb', 1.0], ['baababaaaaaababbababb', 0.0], ['aaa', 0.0], ['ab', 1.0], ['aaababbaaabaabaaaaaabaabbab', 0.0], ['bbbbaaaabbbabbb', 0.0], ['aabbaba', 0.0], ['ababbab', 0.0], ['bbaba', 0.0], ['aabaabaaabaaaaabababba', 1.0], ['ababbbbaababbbb', 0.0], ['abbbaaa', 0.0], ['bba', 0.0], ['baa', 0.0], ['abbaabbaabaaabbabaaaaab', 0.0], ['baaab', 0.0], ['bba', 0.0], ['baababa', 0.0], ['babaaaabbaaaaabaaabaaba', 0.0], ['baaaba', 1.0], ['aaabbbbbaabbbaaa', 1.0], ['aa', 1.0], ['ababbbabaababababbaaa', 0.0], ['bbaabbaaaaaaaaabbaababbaab', 1.0], ['bbabab', 1.0], ['abababb', 0.0], ['bbbbbaa', 0.0], ['a', 0.0], ['baabbbbbabababaabaabbaaabba', 0.0], ['bbbaabaabbbb', 1.0], ['baabbaabbbbbbbbaba', 1.0], ['abbbbaa', 0.0], ['baababbabaabbaaaaaaaaabbbaaa', 1.0], ['bbabb', 0.0], ['abbabaa', 0.0], ['abbabababbb', 0.0], ['abaaa', 0.0], ['aabbabbab', 0.0], ['bbabaabb', 1.0], ['bbaaabaababababab', 0.0], ['baaaaaaa', 1.0], ['bababbba', 1.0], ['baaaab', 1.0], ['b', 0.0], ['aabaaaabab', 1.0], ['babbbaaabbbaabbbb', 0.0], ['baaba', 0.0], ['ab', 1.0], ['abbbaaab', 1.0], ['abbbabaa', 1.0], ['abbb', 1.0], ['ba', 1.0], ['aabababaaba', 0.0], ['babbbbaaabaaaaa', 0.0], ['baabaaabbaa', 0.0], ['b', 0.0], ['ba', 1.0], ['babbabaa', 1.0], ['baaab', 0.0], ['b', 0.0], ['baaabba', 0.0], ['bbaaa', 0.0], ['abbaabaabbaaaba', 0.0], ['bbbbaba', 0.0], ['babbbaaaaabaabbaabaaa', 0.0], ['abaabb', 1.0], ['aabbbabbbaabaabbbbabbabaa', 0.0], ['aaaabbabbb', 1.0], ['aabbaaaababbbbb', 0.0], ['baababbbbaaaaaabababbbb', 0.0], ['aaba', 1.0], ['ab', 1.0], ['aababbbabbbbbbabb', 0.0], ['aaabbbbbaabaaabbbaabbbabbbab', 1.0], ['bbbabbaabab', 0.0], ['aaabaaaaaaabaabaaaaaaaaab', 0.0], ['abbbbbabbabababba', 0.0], ['abbabbaabaabaaabaabbabb', 0.0], ['bbaba', 0.0], ['babba', 0.0], ['abbba', 0.0], ['bbaabbbaaab', 0.0], ['babba', 0.0], ['bbaaabba', 1.0], ['a', 0.0], ['b', 0.0], ['abbab', 0.0], ['aaaabbbabbbaaabab', 0.0], ['baabbaaaaabaabbbbbbbaabb', 1.0], ['abbaaaabaaabbaabb', 0.0], ['aabbaabbbababaaababbbb', 1.0], ['baaba', 0.0], ['b', 0.0], ['bbbababbbbaabaaabab', 0.0], ['bbbab', 0.0], ['aaabaabab', 0.0], ['babaab', 1.0], ['baababaaaba', 0.0], ['babaabbabbaaaa', 1.0], ['ababaabbbbbbbabaaaaababb', 1.0], ['aababbab', 1.0], ['aabaaaaabaaaabbabab', 0.0], ['bbabbbbbaabb', 1.0], ['bbb', 0.0], ['ab', 1.0], ['baabbbaabaabaaaabbaabb', 1.0], ['a', 0.0], ['bbaaaaabbabaaababaaaab', 1.0], ['b', 0.0], ['ab', 1.0], ['bbbbb', 0.0], ['bba', 0.0], ['aaa', 0.0], ['babaabaabbbaaaaaa', 0.0], ['babbaa', 1.0], ['bbbaaaababaaaaaabaabbbabbba', 0.0], ['bababbbbaabaabbaabbababbbaaab', 0.0], ['baabababbabaaaaaaa', 1.0], ['baa', 0.0], ['aaa', 0.0], ['abbaaabababbabbabaabbabaaaabb', 0.0], ['baabbbbbaaaa', 1.0], ['bbbbaaaababa', 1.0], ['aa', 1.0], ['aaaaabbbabb', 0.0], ['baababaa', 1.0], ['aab', 0.0], ['abbabaaaababbbbbbab', 0.0], ['abbbbaabbbaa', 1.0], ['aa', 1.0], ['aabbaaaa', 1.0], ['babb', 1.0], ['bab', 0.0], ['baaaa', 0.0], ['bababababaabbaabaabaa', 0.0], ['b', 0.0], ['aabbaabbabbbbbabba', 1.0], ['bbbaabba', 1.0], ['ababa', 0.0], ['bbaaabb', 0.0], ['aaaaaa', 1.0], ['bbabaaabaabbababaabababbb', 0.0], ['baa', 0.0], ['bb', 1.0], ['bbbbaabbb', 0.0], ['babaaa', 1.0], ['aabbbb', 1.0], ['abbaababbabaaab', 0.0], ['ababaabaabaababbbaab', 1.0], ['baaa', 1.0], ['bbbabababababababaababaababb', 1.0], ['bbbbbaa', 0.0], ['aaabbabaaabbbbaaaabbbbbaab', 1.0], ['baaaaabb', 1.0], ['a', 0.0], ['bbbaaaaaa', 0.0], ['bbab', 1.0], ['bbbaabbaaba', 0.0], ['aaabbaaababababbbbaaabb', 0.0], ['ababaabbbbbabbaabbabbbbbbbaa', 1.0], ['bababaaabaaabbbbaababaaabaa', 0.0], ['aaabbbbbbbabbaa', 0.0], ['ababbbaabaabbaaabaaa', 1.0], ['bababbabbaa', 0.0], ['aababaabb', 0.0], ['abaaaabba', 0.0], ['bbaabbbbababaa', 1.0], ['bba', 0.0], ['baaa', 1.0], ['baa', 0.0], ['aaababbbaaabbabbbab', 0.0], ['bbaaaba', 0.0], ['abbba', 0.0], ['bababbaabaaab', 0.0], ['bbabbabaabbbbbb', 0.0], ['abbbbb', 1.0], ['aabba', 0.0], ['aaabbaaab', 0.0], ['bbb', 0.0], ['abaab', 0.0], ['abaaa', 0.0], ['abaaabbbbbbaabbaaaaabbbabab', 0.0], ['bbbbbbab', 1.0], ['bbbbababb', 0.0], ['aaaaabb', 0.0], ['abbbaabbaaabbaaaabaabaa', 0.0], ['aaab', 1.0], ['babaaaa', 0.0], ['bbb', 0.0], ['aa', 1.0], ['bbb', 0.0], ['babaaaaaabababaaabab', 1.0], ['aaaaaabaaaabbbaabaa', 0.0], ['aabbbabbaaabababbbbaabbbabb', 0.0], ['abbbbaaaaaaaaba', 0.0], ['ababbaa', 0.0], ['aaaabbaaabaabaa', 0.0], ['bbb', 0.0], ['abbbb', 0.0], ['aabaab', 1.0], ['baa', 0.0], ['babb', 1.0], ['aaabab', 1.0], ['abaabbb', 0.0], ['baabbabba', 0.0], ['bbbba', 0.0], ['abaaaaaabbabaabbbbb', 0.0], ['aaaaa', 0.0], ['abbbbbabbbba', 1.0], ['aa', 1.0], ['abb', 0.0], ['aaaaabbababbabbbbabaababbb', 1.0], ['abbabababaabbbaba', 0.0], ['baaaaabbbaab', 1.0], ['baababaaabababaaab', 1.0], ['bab', 0.0], ['bbbabbaaabbbbaababb', 0.0], ['bbbab', 0.0], ['bbaa', 1.0], ['bbbbababa', 0.0], ['bbbbaba', 0.0], ['abbababbabaaaaab', 1.0], ['abbbbbab', 1.0], ['bbabaaaababbababababa', 0.0], ['bab', 0.0], ['aabaabab', 1.0], ['abaabbababaabaabba', 1.0], ['abbabaabaaabbaaabbba', 1.0], ['babbbbbb', 1.0], ['aab', 0.0], ['abaabababaaaaaabbbbbb', 0.0], ['baaabbbab', 0.0], ['abbaaaba', 1.0], ['aaabaa', 1.0], ['bababbaab', 0.0], ['aabb', 1.0], ['babbbbbbbba', 0.0], ['bbabbbabbbbbbbaaaababa', 1.0], ['aabbabb', 0.0], ['aaaabbaaabba', 1.0], ['bbbab', 0.0], ['bbabbabbbbaabbbabbbbbababbaab', 0.0], ['babbbbaabbbbaa', 1.0], ['bbbaaa', 1.0], ['aaaabbbabababbbabaaba', 0.0], ['babbbbbaabbab', 0.0], ['bbabbaa', 0.0], ['b', 0.0], ['baaaaabbab', 1.0], ['abbaa', 0.0], ['bbbaababbbabaaab', 1.0], ['bbbaaaabbaabbbaaa', 0.0], ['bbbbababbababababbaa', 1.0], ['aaabb', 0.0], ['abbaabaababbbbababbbaabaa', 0.0], ['babbaa', 1.0], ['ababbaabbbaabbababbaaaaabba', 0.0], ['bbbabbabbaaabbabbbab', 1.0], ['bbbbaabbbbaabbb', 0.0], ['aabbaababbaabbababbababb', 1.0], ['bbabaabbaabbabababbaaaaabab', 0.0], ['abbbb', 0.0], ['abbbb', 0.0], ['bbaababaabb', 0.0], ['baabbaabbbabb', 0.0], ['aa', 1.0], ['abbaaaa', 0.0], ['bbbaaaaabbbbaaaabba', 0.0], ['aabbaaaabaab', 1.0], ['ab', 1.0], ['abaaaaa', 0.0], ['babaaab', 0.0], ['aabababaaaababa', 0.0], ['abbaa', 0.0], ['abab', 1.0], ['abbbababbaababbbb', 0.0], ['ababbbabbaa', 0.0], ['bbaaab', 1.0], ['abbabba', 0.0], ['a', 0.0], ['abaababbabaabaaab', 0.0], ['baaaabaaabaabb', 1.0], ['bbbabaaaabb', 0.0], ['aabb', 1.0], ['aaab', 1.0], ['baaabaabbababa', 1.0], ['bababbabbbbabaaabbaaab', 1.0], ['babbaaaabbbaaabbaababbbbaabab', 0.0], ['bababbbbbabbbbbaaabba', 0.0], ['bbbabbabaaa', 0.0], ['bababaababbb', 1.0], ['bbbabaaababb', 1.0], ['babaaba', 0.0], ['bbaaab', 1.0], ['baaa', 1.0], ['aabbaab', 0.0], ['bbbaba', 1.0], ['aaabbaa', 0.0], ['ababb', 0.0], ['bbbbaaab', 1.0], ['abaabbababbabbbbaabba', 0.0], ['baaaaaabaabbbbbbaaaabaabaaab', 1.0], ['baababababaabbbaba', 1.0], ['babbaaababbaaababaaab', 0.0], ['aaaaababbbbbaaa', 0.0], ['aba', 0.0], ['bbbbaaabbbaababaababbbaabaabb', 0.0], ['aabaaabaabbaabaaabba', 1.0], ['abbbbbbaaba', 0.0], ['aabbb', 0.0], ['aaaaabbb', 1.0], ['abbabaaaaabbabababbba', 0.0], ['aaababababaaabaaaabbbbbba', 0.0], ['aba', 0.0], ['aababbbbabbaa', 0.0], ['baabab', 1.0], ['a', 0.0], ['bbbbaaab', 1.0], ['aba', 0.0], ['bb', 1.0], ['abbaaa', 1.0], ['ababbba', 0.0], ['aa', 1.0], ['aaaaabbbaab', 0.0], ['bbbababb', 1.0], ['aabbbbaa', 1.0], ['aabbabaa', 1.0], ['aabbaabbabbaabaabbabaa', 1.0], ['baaabbaa', 1.0], ['abaabbbabbabbbaaabbbababaaaab', 0.0], ['aaaabaabaaaab', 0.0], ['babbabbaabababbaaaababa', 0.0], ['abbaaaba', 1.0], ['ba', 1.0], ['aabaabb', 0.0], ['abb', 0.0], ['baaabbbb', 1.0], ['abaa', 1.0], ['babb', 1.0], ['bb', 1.0], ['abaaaab', 0.0], ['abaabbaaababbababbbbbb', 1.0], ['a', 0.0], ['a', 0.0], ['bbbbbbbaaabbbbaababaaabbb', 0.0], ['aaaabaab', 1.0], ['babbbbbbbbbbbaaabbaabbbb', 1.0], ['aaaabbabaabaaa', 1.0], ['bbb', 0.0], ['baaaaabbabbaaabaaaaaabbabbbb', 1.0], ['abababa', 0.0], ['babbaaabbababbab', 1.0], ['babaabbbbbbbaaaabaaababbaaa', 0.0], ['baaaaaab', 1.0], ['ba', 1.0], ['bbbbbbbb', 1.0], ['ababbaaaaaaabbbbabbabaa', 0.0], ['bb', 1.0], ['ababbbbbabbaabbaabaababbb', 0.0], ['bbbbbabbaab', 0.0], ['baaaabaaa', 0.0], ['b', 0.0], ['bbaab', 0.0], ['bbaabbab', 1.0], ['aaabb', 0.0], ['b', 0.0], ['a', 0.0], ['aabab', 0.0], ['ba', 1.0], ['a', 0.0], ['b', 0.0], ['babbbbaaabbabbba', 1.0], ['aaababababbaabbbaaaaaaa', 0.0], ['bbbbabbaaa', 1.0], ['bab', 0.0], ['bbbbabaab', 0.0], ['aaaabbaa', 1.0], ['bbbbababbbaba', 0.0], ['a', 0.0], ['ababb', 0.0], ['baa', 0.0], ['bbaababaa', 0.0], ['babbbab', 0.0], ['bbaabbba', 1.0], ['bab', 0.0], ['aabbabbbaaaaabbbabababbbbbbba', 0.0], ['aaaabababba', 0.0], ['b', 0.0], ['aabbab', 1.0], ['bbaba', 0.0], ['a', 0.0], ['bba', 0.0], ['abbabbaaabb', 0.0], ['bbbbbaabbaaaabbabbbaa', 0.0], ['aababaabbbbbbbbaabbabbbabaabb', 0.0], ['b', 0.0], ['aabbaaabbbb', 0.0], ['aabb', 1.0], ['baa', 0.0], ['a', 0.0], ['bbaaabab', 1.0], ['aaaaab', 1.0], ['a', 0.0], ['abb', 0.0], ['bbaabbbaba', 1.0], ['bababba', 0.0], ['ababbababaabbba', 0.0], ['bbbaaaaabbaabbbab', 0.0], ['aabaaaaabbbabbbbbb', 1.0], ['ababba', 1.0], ['b', 0.0], ['bbabbbaaabb', 0.0], ['aaab', 1.0], ['ababaa', 1.0], ['bbaaaaa', 0.0], ['aaababbaababaaabbbbaaa', 1.0], ['b', 0.0], ['abbabbbaabaabbbab', 0.0], ['bbabbabbbabbbabbba', 1.0], ['abaabbbabbaabbaabaaabaa', 0.0], ['baaabaabaaaaa', 0.0], ['bbaaaabaaabab', 0.0], ['bbababbbaa', 1.0], ['bbbbbbaaaaababbbbab', 0.0], ['abaaba', 1.0], ['babbabbbabbbaabbbbaaabaabbabb', 0.0], ['bbbabb', 1.0], ['aabbaa', 1.0], ['bbabaaabbabaaaa', 0.0], ['a', 0.0], ['b', 0.0], ['bbbabaa', 0.0], ['ababbabbbabbababab', 1.0], ['b', 0.0], ['abababaabbaaabaabbbbabbbbba', 0.0], ['baba', 1.0], ['babaaababaaaa', 0.0], ['bbbbbbbabbaaababaabbaaab', 1.0], ['bb', 1.0], ['bbbbbb', 1.0], ['abaaaaab', 1.0], ['bbaa', 1.0], ['bbaa', 1.0], ['bbaaabbbbbbaaabba', 0.0], ['abbbbbbabaaaaaababaabbbabba', 0.0], ['baba', 1.0], ['bbbbbba', 0.0], ['bbabb', 0.0], ['aabbbbaabbbbaabaaab', 0.0], ['abbaaabbbaababbbbaabaaa', 0.0], ['aba', 0.0], ['bbaaaa', 1.0], ['bbabbababbbaabbabbaabba', 0.0], ['babbabbbbaaabaaaabaabaabaaaa', 1.0], ['bab', 0.0], ['aaabaaa', 0.0], ['abaaabbabaab', 1.0], ['bbbab', 0.0], ['aaabba', 1.0], ['babab', 0.0], ['aabbaaaaaaaabb', 1.0], ['ab', 1.0], ['babababaabb', 0.0], ['abbbbb', 1.0], ['aaaabbbaabaabaaabaabbaabbbaba', 0.0], ['aabaab', 1.0], ['aaabbaaaababababbbaba', 0.0], ['babababaabbab', 0.0], ['babba', 0.0], ['aa', 1.0], ['baabbaabba', 1.0], ['aabbbbabbbaaaaabbbb', 0.0], ['abbaaababaababaaaabbbb', 1.0], ['aab', 0.0], ['aaa', 0.0], ['bbbbaabaabababbabaaaabaa', 1.0], ['baaaabbaaa', 1.0], ['bbbaaaaabbbabbab', 1.0], ['abbaabb', 0.0], ['bbbabba', 0.0], ['baaaabbbb', 0.0], ['baaaabbbbabbaaababbaaba', 0.0], ['aba', 0.0], ['baaabbaabbbbbabaaaababbba', 0.0], ['bbbbbbabb', 0.0], ['bbabbabbabaaa', 0.0], ['abaaabbbbaabbabababbbbbaa', 0.0], ['bbbb', 1.0], ['abaaaabbbaabaabbbaababbbaba', 0.0], ['baababbbabbba', 0.0], ['aaabaaaaabbabbb', 0.0], ['aabbbbbaaa', 1.0], ['baba', 1.0], ['babaaaabbabbab', 1.0], ['baaaa', 0.0], ['b', 0.0], ['bbabbbbbbbbaaaaaabbb', 1.0], ['b', 0.0], ['abbbabbbababbba', 0.0], ['aabbbaababaaabbaabb', 0.0], ['aaa', 0.0], ['bbbaaab', 0.0], ['bbbbabaaaabbaababbb', 0.0], ['abaaabbabbbbaab', 0.0], ['abba', 1.0], ['aababaaabb', 1.0], ['baa', 0.0], ['a', 0.0], ['aababababbabbbba', 1.0], ['ba', 1.0], ['aaab', 1.0], ['ababaaabbbbab', 0.0], ['aaaaabbbbbbb', 1.0], ['bbbbbaab', 1.0], ['b', 0.0], ['bbbbabbbaaa', 0.0], ['aabbabaa', 1.0], ['aaabaababbaab', 0.0], ['bbaaba', 1.0], ['abbbaaaab', 0.0], ['abababb', 0.0], ['baaba', 0.0], ['abaabbba', 1.0], ['aabaabbabababababbabaabab', 0.0], ['bbababaaaaaaababbaabbaaabba', 0.0], ['aaaab', 0.0], ['babaaaba', 1.0], ['aaaaaabbabbaaabbbaa', 0.0], ['aabbaababbbaa', 0.0], ['aabaabbabaaba', 0.0], ['aabbabbaaaaabba', 0.0], ['baabbbaaaabbabbabba', 0.0], ['ababaabb', 1.0], ['abaaaaabaababababbbababbab', 1.0], ['aaaabaab', 1.0], ['baaabaaaaba', 0.0], ['aabaabab', 1.0], ['bbbba', 0.0], ['bbbbab', 1.0], ['bbabbabaaaaababbbabb', 1.0], ['aaaa', 1.0], ['aaaabbababbbbabbbbbbbaa', 0.0], ['abbaab', 1.0], ['aabaaabbbbaaabaaabaabbb', 0.0], ['baaaabbbaaaabbbb', 1.0], ['bbaab', 0.0], ['aa', 1.0], ['aabbaaa', 0.0], ['babaaa', 1.0], ['aabbbabaabbbbababaa', 0.0], ['bbbbabbaabbaabbbaaaabb', 1.0], ['aaaba', 0.0], ['aaabbaabbabbb', 0.0], ['babbbabaaab', 0.0], ['a', 0.0], ['bbabbb', 1.0], ['ba', 1.0], ['bbbbbbbababbabababaaa', 0.0], ['abbbbbbbabbbbbaaaba', 0.0], ['bba', 0.0], ['abbaabbbabaabbbbaaba', 1.0], ['bb', 1.0], ['bbabaaababb', 0.0], ['abaa', 1.0], ['baaababbbbaabbbbbbaa', 1.0], ['abbb', 1.0], ['ababb', 0.0], ['abbabbabaababbbabaaababba', 0.0], ['aaba', 1.0], ['baabbbbbabbabbbaaaabb', 0.0], ['bba', 0.0], ['bbabbba', 0.0], ['aaa', 0.0], ['bbbbaaba', 1.0], ['ababbbbaa', 0.0], ['bbbabbbb', 1.0], ['abbbb', 0.0], ['bbbbabababbaaabbabbbbbb', 0.0], ['abbbabaab', 0.0], ['abaaabbababbbbbbaaababbbabba', 1.0], ['abbbbaaa', 1.0], ['bbabbababaababbbbbbb', 1.0], ['aaabbaaabba', 0.0], ['bababbaabaaaaabbabaaaabab', 0.0], ['bbaabbaababbbaabbabbbbba', 1.0], ['abaabbabbbaabbaaa', 0.0], ['aa', 1.0], ['aa', 1.0], ['abaaabb', 0.0], ['a', 0.0], ['aaaaabbaabb', 0.0], ['bbaabaaabaababbbbbaa', 1.0], ['babba', 0.0], ['aaaa', 1.0], ['baabbabbbabbbbaabaaababbb', 0.0], ['bbabbbba', 1.0], ['bbbababbbbaabab', 0.0], ['baaaabb', 0.0], ['baaaba', 1.0], ['aabbabaaaaababbaa', 0.0], ['baababbaabbbbbbabbbaabaa', 1.0], ['bbbabbb', 0.0], ['aaabaab', 0.0], ['bbaaabbabababbabbaaabaabbaaaa', 0.0], ['abba', 1.0], ['bbabababababbbbabbbbb', 0.0], ['aaabaaabbabaa', 0.0], ['bbaabbaba', 0.0], ['babaabbababbbabbab', 1.0], ['bb', 1.0], ['bbbaaababaa', 0.0], ['bb', 1.0], ['baabbaa', 0.0], ['baaabb', 1.0], ['a', 0.0], ['aa', 1.0], ['aabaabbbbaaaaabbabbababab', 0.0], ['baaaaaa', 0.0], ['abbaabbbaabbabaaabbbbbaa', 1.0], ['bbbabbbabbaaabbbaba', 0.0], ['ab', 1.0], ['bbb', 0.0], ['aaaaa', 0.0], ['ab', 1.0], ['bbabbbababbaaabbbbbab', 0.0], ['bbababa', 0.0], ['aaabababaabba', 0.0], ['baabbaba', 1.0], ['aabaa', 0.0], ['b', 0.0], ['bbbabb', 1.0], ['aa', 1.0], ['aabbaababaaaaab', 0.0], ['bbbbbaabb', 0.0], ['bbaaabbb', 1.0], ['babbaa', 1.0], ['babaaaaababbaaabbbabbbabbabb', 1.0], ['a', 0.0], ['aaaa', 1.0], ['ababaaabbbbbaabbb', 0.0], ['bbbbba', 1.0], ['aababbba', 1.0], ['bbaabaabbbaabbaabbababb', 0.0], ['bbbabaabbbaababababaaaaa', 1.0], ['ababbbbb', 1.0], ['bbaabaababbbaababbaaaab', 0.0], ['b', 0.0], ['aabbabaaa', 0.0], ['aaabaabbabababaabbbb', 1.0], ['ab', 1.0], ['aba', 0.0], ['ba', 1.0], ['a', 0.0], ['aa', 1.0], ['bbbabaabbaaaaaaabba', 0.0], ['aba', 0.0], ['ab', 1.0], ['ababab', 1.0], ['bab', 0.0], ['a', 0.0], ['bbbbbababaabababbbaab', 0.0], ['bbaaaabaaabbababa', 0.0], ['baba', 1.0], ['bb', 1.0], ['bb', 1.0], ['baaabab', 0.0], ['aaaa', 1.0], ['b', 0.0], ['bbbba', 0.0], ['bbbbabbbaabbbbaaaaaaabbaa', 0.0], ['babbbabaabbaaaaba', 0.0], ['aaababbb', 1.0], ['ab', 1.0], ['bab', 0.0], ['abaaaaaaabbbabbbbabaaaabba', 1.0], ['b', 0.0], ['baabbaa', 0.0], ['abaabaaaababa', 0.0], ['baabbbaabb', 1.0], ['abbbaababbabbbabaabbabbaabaa', 1.0], ['baaabaaaabaabaaaa', 0.0], ['babbbbba', 1.0], ['baaa', 1.0], ['aaabaaba', 1.0], ['bbaa', 1.0], ['bbab', 1.0], ['abaaabbbaaaababbb', 0.0], ['ba', 1.0], ['aabaab', 1.0], ['ab', 1.0], ['baabaaaaabaabababababbaabaa', 0.0], ['babbbbbbaababba', 0.0], ['baaabaa', 0.0], ['abbbbaaaababaaaabababbabb', 0.0], ['aabbb', 0.0], ['baaaa', 0.0], ['b', 0.0], ['aaaaaaab', 1.0], ['aaa', 0.0], ['aa', 1.0], ['aaabba', 1.0], ['b', 0.0], ['b', 0.0], ['bbbbabbaa', 0.0], ['bab', 0.0], ['baa', 0.0], ['baaabbaaabababaa', 1.0], ['baaabbbbabbbabab', 1.0], ['baabbabb', 1.0], ['abaabbaabbabbabbabababbbb', 0.0], ['aabaabaaababababbbabab', 1.0], ['aabaabbbbbbab', 0.0], ['a', 0.0], ['abaabbbbbbabaaababaaaaaaab', 1.0], ['aa', 1.0], ['bbababba', 1.0], ['aaa', 0.0], ['bbbbbbaababbbbaabaaaabaabaa', 0.0], ['aabaa', 0.0], ['aba', 0.0], ['bbbbabaabbaa', 1.0], ['aa', 1.0], ['abaababbbaaabaaabaaaaabbabbba', 0.0], ['babbb', 0.0], ['bbbaababaababbbaaa', 1.0], ['bbaababbaabaabbbbbb', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of alphabets set according to the alphabet size, it is known before constructing a DFA\n",
    "# L = {w has no same neighboring characters}\n",
    "\n",
    "DICTIONARY = ['a', 'b'] \n",
    "NUMBER_OF_CHARS = len(DICTIONARY)              # number of alphabets, |A|\n",
    "\n",
    "# getting the dataset of the language\n",
    "f = open('dataset/tl9_dataset.txt', 'r')  # get the dataset for a language\n",
    "dataset = []                              # array to hold the (w,ans) \n",
    "for line in f:\n",
    "    arr = line.split(' ')\n",
    "    ans = 1.0\n",
    "    if arr[1][0] == '0':\n",
    "        ans = 0.0\n",
    "    dataset.append([arr[0], ans])\n",
    "\n",
    "\n",
    "random.shuffle(dataset)\n",
    "print(dataset)\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-compatibility",
   "metadata": {},
   "source": [
    "### definition of all the function used in the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bored-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function (y_true - y_predicted)^2\n",
    "def cost_function(exp, res):                   \n",
    "    return (res - exp) ** 2\n",
    "\n",
    "# derivative of loss function (ytrue - yloss)\n",
    "def cost_function_derivative(exp, res):       \n",
    "    return res - exp\n",
    "\n",
    "\n",
    "# Match function: takes the nn tensor, current state and the next character and performs the transition\n",
    "# ch is the next character and pos is the current position\n",
    "# Q x S > Q i.e next state = tensor x next character x current state\n",
    "def match(nn, ch, pos):                         \n",
    "    new_pos = np.zeros(NUMBER_OF_POSITIONS)         # create a new array for the next state\n",
    "    for k in range(NUMBER_OF_POSITIONS):            # for each value in the new state\n",
    "        for i in range (NUMBER_OF_CHARS):           # go through next character array\n",
    "            for j in range (NUMBER_OF_POSITIONS):   # go through current staste array\n",
    "                new_pos[k] += nn.tensor[k][i][j] * ch[i] * pos[j] # new state position is tensor x next character x current state\n",
    "    return new_pos                                  # return the new state\n",
    "\n",
    "# derivative of Match function wrt current state: takes the tensor, a gradient and next input character\n",
    "# derivative (2D array)= tensor x next character\n",
    "def match_derivative(nn, dz, ch):\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_POSITIONS]) # dim: |Q| x |Q|\n",
    "    for i in range(NUMBER_OF_POSITIONS):                   # for each column in derivative i.e values in current state [p1 p2 p3 ..]\n",
    "        for k in range(NUMBER_OF_POSITIONS):               # for each row in derivative i.e vlues in new state [h1 h2 h3 ..]\n",
    "            for j in range (NUMBER_OF_CHARS):              # loop through current character  \n",
    "                derivative[k][i] += nn.tensor[k][j][i] * ch[j] # derivative (2D array)= tensor x next character\n",
    "    return np.dot(dz, derivative)                          # dot product of previous gradient with new one\n",
    "\n",
    "# derivative of Match function wrt tensor: takes a gradient, next input character and current state \n",
    "# derivative(3D array) = next character x current state \n",
    "def match_derivative_tensor(dz, ch, pos):\n",
    "    sample_matrix = np.zeros([NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # create a two 2D array of next character and current state\n",
    "    for i in range(NUMBER_OF_CHARS):\n",
    "        for j in range(NUMBER_OF_POSITIONS):\n",
    "            sample_matrix[i][j] = ch[i] * pos[j]                     # fill the sample matrix vy multiplying next character and current state\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # new derivative as a 3D array\n",
    "    for k in range(NUMBER_OF_POSITIONS): # for each row in first dimension\n",
    "        derivative[k] = dz[k] * sample_matrix      # new derivative is old * sample matrix\n",
    "    return derivative                              # return derivative\n",
    "\n",
    "\n",
    "# normalize function for each value in h_k return h/sum(h)\n",
    "def normalize(h):                    \n",
    "    return h / np.sum(h)\n",
    "\n",
    "# derivative of normalize: takes a derivative, and input(h) to the output layer\n",
    "def normalize_derivative(dz, h):\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_POSITIONS])\n",
    "    sum = np.sum(h)\n",
    "    for i in range(NUMBER_OF_POSITIONS):      # for each value in the output layer vector \n",
    "        for j in range(NUMBER_OF_POSITIONS):  # for each value in the output layer vector\n",
    "            if(i == j):                       # if position of value wrt to which derivative is being taken is equal, \n",
    "                derivative[i][j] = (sum - h[i]) / (sum ** 2) # derivative is (sum - value)/sum^2\n",
    "            else:\n",
    "                derivative[i][j] = -h[i] / (sum ** 2)        # else -value/sum^2\n",
    "    return np.dot(dz, derivative)             # dot product of previous derivative with new one\n",
    "\n",
    "\n",
    "# applying adder function to output layer, takes the adder and the final state\n",
    "# final output neuron = adder x final state\n",
    "def lastsum(nn, x):\n",
    "    return np.dot(nn.adder, x) # final output neuron = adder x final state\n",
    "\n",
    "# derivative of adder wrt its argument i.e final output vector,to be used in tensor backpropagation \n",
    "# takes adder and a derivative\n",
    "def lastsum_derivative(nn, dz):\n",
    "    return np.dot(dz, nn.adder)      # dot product of adder values(k) and current derivative\n",
    "\n",
    "# derivative of adder wrt to k and output vector, to be used in adder backpropagation\n",
    "# takes adder, current deriviative and a output layer state\n",
    "def lastsum_derivative_adder(nn, dz, inp):\n",
    "    derivative = np.multiply(inp, nn.adder) # derivative is output layer state x adder \n",
    "    return np.dot(dz, derivative)\n",
    "\n",
    "# function to change a character to vector encodings\n",
    "def char_to_vector(ch):\n",
    "    index = DICTIONARY.index(ch)    # get the index of a character  from dictionary\n",
    "    vec = np.zeros(NUMBER_OF_CHARS) # get a 1D array of len(alphabet) x 1\n",
    "    vec[index] = 1.0                # change the index of character character to 1\n",
    "    return vec                      # return the vector\n",
    "\n",
    "# cut function which rounds up the value to 0 or 1, weights should be between 0 and 1\n",
    "def cut(x):\n",
    "    if (x > 1.0):        # if x is greater than 1, round it to 1\n",
    "        return 1.0\n",
    "    if (x < 0.0):\n",
    "        return 0.0       # if x is lesser than 0. round it to 0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-retailer",
   "metadata": {},
   "source": [
    "### RNN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "injured-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network object with two attributes and three methods\n",
    "# two attributes are tensor and adder which are alterable parameters of the network, tensor and adder\n",
    "# three methods are check, train, train_online, get_automaton, and run\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # dim: |Q| x |A| x |Q|, Q is number of states and |A| is alphabet size\n",
    "    tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS])  \n",
    "    adder = np.zeros(NUMBER_OF_POSITIONS)\n",
    "\n",
    "    def __init__(self): # init method that creates tensor and adder attribute and initializes it\n",
    "        \n",
    "        self.tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # dim: to x ch x fr\n",
    "        for fr in range(NUMBER_OF_POSITIONS):  # for each index in fr state\n",
    "            for ch in range(NUMBER_OF_CHARS):  # for each index in a character\n",
    "                z = np.random.rand(NUMBER_OF_POSITIONS) # get a random value intialized array of lenght |Q|\n",
    "                z = normalize(z)                        # normalize it \n",
    "                for to in range(NUMBER_OF_POSITIONS):   # for each index in to state\n",
    "                    self.tensor[to][ch][fr] = z[to]     # initialize the given index of tensor\n",
    "                    \n",
    "        self.adder = np.zeros(NUMBER_OF_POSITIONS)      # dim: |Q| x 1\n",
    "        for to in range(NUMBER_OF_POSITIONS):           # for each index in the adder array\n",
    "            # initialize each index with a value corresponding to its index, value = (index + 1)/|Q|\n",
    "            self.adder[to] = 1.0 * (to + 1) / (NUMBER_OF_POSITIONS) \n",
    "\n",
    "\n",
    "    def check(self, word): # method to check whether a word belongs to the language or not\n",
    "        curr_pos = START_POSITION                       # start from start position \n",
    "        for k in range(len(word)):                      # for each character in the word\n",
    "            curr_word = char_to_vector(word[k])         # convert it to vector form \n",
    "            curr_pos = match(self, curr_word, curr_pos) # make a transition \n",
    "            curr_pos = normalize(curr_pos)              # normalize the next state\n",
    "        output = lastsum(self, curr_pos)                # then apply an adder function to get a final output\n",
    "        return output\n",
    "\n",
    "    def train_online(self, dataset): # method to online train the network i.e weight are adjusted after every word is read\n",
    "        average_error = 1.0          # initializing and average error\n",
    "        epoch_number = 0             # epoch number\n",
    "        n = len(dataset)             # number of words in the datset\n",
    "        tests_size = int(PERCENT_OF_TESTS * n) # number of words allocated for testing the dataset\n",
    "        while (average_error > EPS):           # while the error is greater than specified epsilon  \n",
    "            random.shuffle(dataset)            # shuffle the datset for better training\n",
    "            cases_left = len(dataset)          # varaible to track the number of words lefts to train on\n",
    "            epoch_number += 1                  # increment the epch by 1\n",
    "            print ( 'Epoch #' + str(epoch_number))  # print epoch number\n",
    "            while(cases_left > tests_size):         # for a epoch train on the whole dataset \n",
    "                self.train(dataset[cases_left - 1][0], dataset[cases_left - 1][1]) # training is done from the last word\n",
    "                cases_left -= 1\n",
    "            average_error = 0.0                    \n",
    "            for i in range(cases_left):         # for each word in the dataset \n",
    "                average_error += cost_function(dataset[i][1], self.check(dataset[i][0])) # predict and calculate the error\n",
    "            average_error /= cases_left         # normalize the average error\n",
    "            print (\"Average error: \" + str(average_error)) # print the average error\n",
    "\n",
    "    def train(self, word, exp):    # method to train on a word (string), takes word and expected output(0 or 1)\n",
    "        cut_v = np.vectorize(cut)  # vectorize cut function as cut_v\n",
    "        word_length = len(word)    # length of the word to be trained on\n",
    "        \n",
    "        # array to track the states the input character leads to\n",
    "        positions = np.zeros([word_length + 1, NUMBER_OF_POSITIONS])     # dim: (word length + 1) x number of states\n",
    "        \n",
    "        # array to store the values after a character of a word has been consumed, i.e the raw values of the next state \n",
    "        # after transition t is applied on current state and next input character\n",
    "        before_normalize = np.zeros([word_length, NUMBER_OF_POSITIONS])  # dim: word length  x number of states\n",
    "        \n",
    "        # array to store the derivative of transition tensor \n",
    "        d_tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # dim: |Q| x |A| x |Q|\n",
    "        \n",
    "        # array to store the derivative of adder \n",
    "        d_adder = np.zeros(NUMBER_OF_POSITIONS) # dim: |Q| x 1\n",
    "        positions[0] = START_POSITION           # 0th index of position is start position\n",
    "        \n",
    "        #  do a forward pass on the network i.e match and normalize until the end of the word, and then adder at the end\n",
    "        for k in range(word_length):             # for each character in word\n",
    "            curr_word = char_to_vector(word[k])  # change it to vector form                  \n",
    "            before_normalize[k] = match(self, curr_word, positions[k]) # apply match on it and save the raw output\n",
    "            positions[k + 1] = normalize(before_normalize[k])          # normalize the raw output and save the next state\n",
    "        answer = lastsum(self, positions[-1])    # compute the adder on final output from normalize, get the final value\n",
    "        \n",
    "        error = cost_function(exp, answer)       # computer the cost on obtain answer (y_true - y_predicted)^2 \n",
    "\n",
    "\n",
    "        gradient = cost_function_derivative(exp, answer)  # compute the gradient of the error\n",
    "        \n",
    "        # gradient calculation for adder backpropagation\n",
    "        d_adder += lastsum_derivative_adder(self, gradient, positions[-1]) # compute the gradient of adder wrt its weights\n",
    "        \n",
    "        # gradient calculation for tensor backpropagtion\n",
    "        gradient = lastsum_derivative(self, gradient)     # calculate the derivative of adder wrt final output vector\n",
    "        first_gradient = sum(abs(gradient)) * TenzToAdd   # sum the gradient and store double of it  \n",
    "        \n",
    "        # for tensor, gradient calculation needs to be done w times i.e length of word times.\n",
    "        for k in range(word_length - 1, -1, -1):  # for each character in the word, as we are backtracing       \n",
    "            curr_grad = sum(abs(gradient))        # get the sum of the gradient so far i.e ( error * adder gradient)\n",
    "            if (curr_grad < 0.001):               # if current gradient is very small  < 0.001\n",
    "                koef = 1.0                        # make the coeffiecient as 1\n",
    "            else:\n",
    "                koef = first_gradient / sum(abs(gradient)) # else change the coefficient accordingly\n",
    "            gradient *= koef                      # multiply the coeffiecient with gradient\n",
    "            curr_word = char_to_vector(word[k])   # change the character to vector form, \n",
    "            gradient = normalize_derivative(gradient, before_normalize[k])  # calculate gradient of normalize for the           \n",
    "            \n",
    "            d_tensor += match_derivative_tensor(gradient, curr_word, positions[k]) # calculate the gradient of match \n",
    "                                                  #   wrt its weights and add it to derivative of tensor\n",
    "            gradient = match_derivative(self, gradient, curr_word) # calculate the gradient of match \n",
    "                                                  # wrt its output state\n",
    "\n",
    "        d_tensor /= word_length                   # normalize the d_tensor\n",
    "        self.tensor = cut_v(self.tensor - NU * d_tensor)         # update the tensor weights\n",
    "        self.adder = cut_v(self.adder - NU * NU_ADDER * d_adder) # update the adder weights\n",
    "        return error\n",
    "\n",
    "    def get_automaton(self): # method to get the DFA, traverse through 3D array tensor\n",
    "        \n",
    "        for i in range(NUMBER_OF_POSITIONS): # for each index of a state, last index of 3D array, from state\n",
    "            for j in range(NUMBER_OF_CHARS): # for each index of a character, middle index of 3D array, character\n",
    "                max_ind = 0                  # set max_ind as 0 initially\n",
    "                for k in range(1, NUMBER_OF_POSITIONS): # for each index of a state, first index of 3D array, to state\n",
    "                    if (nn.tensor[k][j][i] > nn.tensor[max_ind][j][i]): # find the index with the maximum value \n",
    "                        max_ind = k                                     # set it to k\n",
    "                print(str(i) + \"--\" + str(DICTIONARY[j]) + '-->' + str(max_ind)) # then from(i) state upon consuming(j)\n",
    "                                                     # has maximum probability of going to to(k) state\n",
    "        \n",
    "        for k in range(NUMBER_OF_POSITIONS):  # for each index of a state in adder vector\n",
    "            if (nn.adder[k] > 0.5):           # if the value of the a particular index is greater than 0.5\n",
    "                print(str(k) + \" is terminal\") # then it has more than 50% chance to be terminal\n",
    "    \n",
    "    \n",
    "    def run(self, word): # method to run a word on the trained dfa and check whether the word gets accepted or rejected\n",
    "        output = nn.check(word)        # call check method\n",
    "        if (output > float(0.6)):      # if the probability is greater than 60 %\n",
    "            ans = 'Accepted'           # then accept the word\n",
    "        else: \n",
    "            ans = 'Rejected'           # or reject the word\n",
    "        return output, ans             # return both output and ans\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compliant-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "Average error: 0.00038250754701583793\n",
      "0--a-->1\n",
      "0--b-->1\n",
      "1--a-->0\n",
      "1--b-->0\n",
      "0 is terminal\n"
     ]
    }
   ],
   "source": [
    "# training the NN and getting the DFA\n",
    "\n",
    "nn = NeuralNetwork();\n",
    "nn.train_online(dataset)\n",
    "nn.get_automaton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "compliant-marine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         1.        ]\n",
      "  [0.         0.92029533]]\n",
      "\n",
      " [[0.9876853  0.        ]\n",
      "  [1.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(nn.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d92e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.02457558]\n"
     ]
    }
   ],
   "source": [
    "print(nn.adder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sapphire-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected\n"
     ]
    }
   ],
   "source": [
    "output, ans = nn.run(\"ababa\")\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
