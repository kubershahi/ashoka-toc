{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "through-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "NUMBER_OF_POSITIONS = 5       # number of states assumed as an upper limit, |Q|\n",
    "\n",
    "# declaring start state\n",
    "START_POSITION = np.zeros(NUMBER_OF_POSITIONS) \n",
    "START_POSITION[0] = 1         # start position is [1 0 0 0]\n",
    "\n",
    "\n",
    "PERCENT_OF_TESTS = 0.1        # percent of data to be used for testing\n",
    "TenzToAdd = 2.0\n",
    "EPS = 0.01                    # error margin to determine successful training\n",
    "NU = 0.7                      # learning rate for tensor\n",
    "NU_ADDER = 0.3                # learning rate for adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "forty-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['acc', 0.0], ['b', 1.0], ['cbb', 0.0], ['a', 1.0], ['bac', 1.0], ['baa', 0.0], ['abc', 1.0], ['bab', 1.0], ['bc', 1.0], ['bc', 1.0], ['c', 1.0], ['bb', 0.0], ['aaa', 0.0], ['a', 1.0], ['ca', 1.0], ['aa', 0.0], ['bba', 0.0], ['bb', 0.0], ['c', 1.0], ['abb', 0.0], ['bcc', 0.0], ['c', 1.0], ['a', 1.0], ['aab', 0.0], ['b', 1.0], ['cbc', 1.0], ['a', 1.0], ['bab', 1.0], ['b', 1.0], ['ab', 1.0], ['bcb', 1.0], ['c', 1.0], ['b', 1.0], ['bcc', 0.0], ['cca', 0.0], ['c', 1.0], ['ba', 1.0], ['cbc', 1.0], ['ab', 1.0], ['cc', 0.0], ['ac', 1.0], ['ac', 1.0], ['ba', 1.0], ['aca', 1.0], ['cbc', 1.0], ['a', 1.0], ['abc', 1.0], ['cab', 1.0], ['ccc', 0.0], ['caa', 0.0], ['bcacab', 1.0], ['acbcabab', 1.0], ['cababa', 1.0], ['acacbcac', 1.0], ['acac', 1.0], ['bcacbcabac', 1.0], ['cbcbaba', 1.0], ['acacacabc', 1.0], ['abcabcb', 1.0], ['baca', 1.0], ['cbacbcbac', 1.0], ['bcac', 1.0], ['acba', 1.0], ['babacb', 1.0], ['babcaba', 1.0], ['acbcbab', 1.0], ['abcacbc', 1.0], ['abacabacba', 1.0], ['cacacb', 1.0], ['cbcac', 1.0], ['abcbaba', 1.0], ['cbcac', 1.0], ['cacbabacab', 1.0], ['acbcbcbcb', 1.0], ['bcab', 1.0], ['ababcbcba', 1.0], ['cacbacbcab', 1.0], ['cbabacbcab', 1.0], ['cbabab', 1.0], ['acbcbac', 1.0], ['cbcbcbacb', 1.0], ['cbcaca', 1.0], ['cabac', 1.0], ['cabcaca', 1.0], ['acacabab', 1.0], ['bacba', 1.0], ['acabacacba', 1.0], ['abcacabac', 1.0], ['acbabca', 1.0], ['babab', 1.0], ['bcbcbacbc', 1.0], ['acabcacb', 1.0], ['acbcabc', 1.0], ['abab', 1.0], ['bcbcaca', 1.0], ['acabc', 1.0], ['bcabcbabac', 1.0], ['cacbcbacb', 1.0], ['cbabacbabc', 1.0], ['acabc', 1.0], ['cabcababab', 1.0], ['acabcbcbac', 1.0], ['abcacacac', 1.0], ['cabacbaca', 1.0], ['bcbca', 1.0], ['acabac', 1.0], ['acab', 1.0], ['bacbcab', 1.0], ['cabcabaca', 1.0], ['abacacabab', 1.0], ['babca', 1.0], ['cbab', 1.0], ['bacbcbc', 1.0], ['acabca', 1.0], ['ababca', 1.0], ['bacbac', 1.0], ['acabcbaba', 1.0], ['caba', 1.0], ['ababab', 1.0], ['bcaca', 1.0], ['cababc', 1.0], ['cabababcba', 1.0], ['abcbacbc', 1.0], ['bcabc', 1.0], ['abacacb', 1.0], ['acac', 1.0], ['cbcacb', 1.0], ['abcbacbcac', 1.0], ['cbac', 1.0], ['cacabab', 1.0], ['cbaba', 1.0], ['abcbcbc', 1.0], ['cabcbabac', 1.0], ['acbc', 1.0], ['cacac', 1.0], ['bcacacb', 1.0], ['caba', 1.0], ['bacac', 1.0], ['acbacacaba', 1.0], ['acbacbacb', 1.0], ['abacabac', 1.0], ['bcacbaba', 1.0], ['acbcbabc', 1.0], ['bcbcbcbaca', 1.0], ['cacacbaba', 1.0], ['cabcbcbabc', 1.0], ['cabcacbc', 1.0], ['babacbaba', 1.0], ['bacbacbaca', 1.0], ['bacacbab', 1.0], ['babcbab', 1.0], ['cbabca', 1.0], ['babcabcba', 1.0], ['bcbabcb', 1.0], ['cbacbabc', 1.0], ['abab', 1.0], ['acacab', 1.0], ['cabacbc', 1.0], ['bcacabc', 1.0], ['bcabca', 1.0], ['cabcbcbac', 1.0], ['cacab', 1.0], ['bababab', 1.0], ['babcacab', 1.0], ['acacabc', 1.0], ['cacbcacabc', 1.0], ['bacbabca', 1.0], ['bcabac', 1.0], ['abcbc', 1.0], ['cacbcbacac', 1.0], ['cbababacab', 1.0], ['cabacbacac', 1.0], ['acbacabcba', 1.0], ['acac', 1.0], ['cbcbabab', 1.0], ['acabacaba', 1.0], ['bcbaba', 1.0], ['ababacacab', 1.0], ['bcacbcac', 1.0], ['acababcba', 1.0], ['cabcb', 1.0], ['cbacbc', 1.0], ['abcabcbac', 1.0], ['bcbcbacb', 1.0], ['abcacac', 1.0], ['cacbcaba', 1.0], ['acabcabc', 1.0], ['bcbabacaca', 1.0], ['cacabcacbc', 1.0], ['cbab', 1.0], ['bacacbaca', 1.0], ['bcba', 1.0], ['abacb', 1.0], ['cbacbabca', 1.0], ['cbaca', 1.0], ['bababcac', 1.0], ['abacb', 1.0], ['bacb', 1.0], ['abcbc', 1.0], ['acabacb', 1.0], ['bcabcbabca', 1.0], ['abcba', 1.0], ['cabab', 1.0], ['cabc', 1.0], ['cacaba', 1.0], ['acbabaca', 1.0], ['cbcabcba', 1.0], ['bacba', 1.0], ['abab', 1.0], ['cacbac', 1.0], ['cabcbcba', 1.0], ['bcbab', 1.0], ['cababacac', 1.0], ['cbcbcba', 1.0], ['abcbc', 1.0], ['bcababa', 1.0], ['babcacabcb', 1.0], ['cbcbacabca', 1.0], ['cbcabcbcb', 1.0], ['bacabca', 1.0], ['bcac', 1.0], ['abcbacabc', 1.0], ['cabcbcab', 1.0], ['bacb', 1.0], ['babcacab', 1.0], ['cabcbacbac', 1.0], ['acacbac', 1.0], ['cbcacbabab', 1.0], ['cacbabca', 1.0], ['cabcb', 1.0], ['acaba', 1.0], ['bababacbc', 1.0], ['acacbca', 1.0], ['acacacac', 1.0], ['cacabcaca', 1.0], ['acacbacbcb', 1.0], ['acbcabcb', 1.0], ['abababa', 1.0], ['babacab', 1.0], ['acabcacaba', 1.0], ['abca', 1.0], ['bacbac', 1.0], ['acbacbcaba', 1.0], ['abcbacbca', 1.0], ['acac', 1.0], ['babababc', 1.0], ['cacacab', 1.0], ['acbacbabc', 1.0], ['cabcabc', 1.0], ['babcb', 1.0], ['cbabcabc', 1.0], ['cabcbcbaba', 1.0], ['cacacabca', 1.0], ['cbcbab', 1.0], ['cbacbcaba', 1.0], ['caca', 1.0], ['acbacb', 1.0], ['babcab', 1.0], ['cbac', 1.0], ['bcabcbcacb', 1.0], ['abacacbc', 1.0], ['babcacb', 1.0], ['abcbcacbac', 1.0], ['cbcbcaca', 1.0], ['cabacbaca', 1.0], ['acabcbaba', 1.0], ['acacaba', 1.0], ['bcabacbaba', 1.0], ['acacbca', 1.0], ['cbabcabc', 1.0], ['acacbabcab', 1.0], ['ababacbab', 1.0], ['cbacba', 1.0], ['cbcacbc', 1.0], ['ababa', 1.0], ['abababa', 1.0], ['abcacb', 1.0], ['bcbaca', 1.0], ['babacbc', 1.0], ['cababcabc', 1.0], ['caca', 1.0], ['acacac', 1.0], ['cabc', 1.0], ['bacbcbabac', 1.0], ['bcab', 1.0], ['acbcabab', 1.0], ['bacacabac', 1.0], ['cacbc', 1.0], ['abcabcbcab', 1.0], ['bacabacbcb', 1.0], ['abaca', 1.0], ['acacbc', 1.0], ['abacba', 1.0], ['cabcab', 1.0], ['bacababcab', 1.0], ['bcbcababab', 1.0], ['acacababcb', 1.0], ['abcbabab', 1.0], ['babcba', 1.0], ['cacbacacba', 1.0], ['abacabc', 1.0], ['abacbacac', 1.0], ['bababc', 1.0], ['abcbcbac', 1.0], ['cbcbcbacab', 1.0], ['abcbc', 1.0], ['abcabcab', 1.0], ['abcba', 1.0], ['cabacbcba', 1.0], ['cacbacbac', 1.0], ['cabab', 1.0], ['cabcbcaba', 1.0], ['bacacbac', 1.0], ['bacacababa', 1.0], ['babcbab', 1.0], ['bcbabcbab', 1.0], ['caba', 1.0], ['bacacb', 1.0], ['ababcbca', 1.0], ['cbacaca', 1.0], ['bacbabc', 1.0], ['acbacacb', 1.0], ['babc', 1.0], ['cbcbacbaca', 1.0], ['abcbacb', 1.0], ['bacabcb', 1.0], ['bacbabcba', 1.0], ['bcacbcb', 1.0], ['cabaca', 1.0], ['abab', 1.0], ['abacb', 1.0], ['bcbacb', 1.0], ['abcabcbca', 1.0], ['bcbacbcaca', 1.0], ['cbabab', 1.0], ['cabcbabcb', 1.0], ['bcbacb', 1.0], ['bababc', 1.0], ['acababcab', 1.0], ['acbac', 1.0], ['acabaca', 1.0], ['abcacbcbab', 1.0], ['cbcbacbcb', 1.0], ['cbcbac', 1.0], ['cacbcbc', 1.0], ['cbcbcac', 1.0], ['cabac', 1.0], ['cabacbca', 1.0], ['acacacbc', 1.0], ['cacbab', 1.0], ['babc', 1.0], ['abacbabcba', 1.0], ['acbcbac', 1.0], ['acabcbac', 1.0], ['bcacacabc', 1.0], ['acbababca', 1.0], ['bababcbab', 1.0], ['abcbabac', 1.0], ['cbab', 1.0], ['cabcbcaca', 1.0], ['ababcac', 1.0], ['cacbcbcaca', 1.0], ['bcbc', 1.0], ['cabcacba', 1.0], ['cbabcbcbac', 1.0], ['bacbacb', 1.0], ['acbabac', 1.0], ['cbcbab', 1.0], ['ababcabcac', 1.0], ['caba', 1.0], ['cacbac', 1.0], ['abcbca', 1.0], ['abcb', 1.0], ['acab', 1.0], ['bcbcba', 1.0], ['bcbc', 1.0], ['caba', 1.0], ['bcac', 1.0], ['bacbaba', 1.0], ['bacbcbcac', 1.0], ['cababacbca', 1.0], ['bacacac', 1.0], ['acab', 1.0], ['bcacacbcab', 1.0], ['bcacbac', 1.0], ['bcbabcbc', 1.0], ['caca', 1.0], ['bcac', 1.0], ['abcbcbca', 1.0], ['cbcbcbabab', 1.0], ['abacb', 1.0], ['bcbacabab', 1.0], ['cabacbc', 1.0], ['abcbacb', 1.0], ['bcabcbacab', 1.0], ['cabacbac', 1.0], ['abacbca', 1.0], ['cbcba', 1.0], ['bacbacbc', 1.0], ['bcab', 1.0], ['cacbab', 1.0], ['cbacb', 1.0], ['cbacbabcb', 1.0], ['acaca', 1.0], ['bacacb', 1.0], ['bababacab', 1.0], ['bcbabacaca', 1.0], ['acac', 1.0], ['abcbca', 1.0], ['cacabcaca', 1.0], ['bacacabc', 1.0], ['abacb', 1.0], ['acacba', 1.0], ['baba', 1.0], ['acba', 1.0], ['cbacacb', 1.0], ['acbcabaca', 1.0], ['cacbabcab', 1.0], ['abcacb', 1.0], ['bcabcac', 1.0], ['bcbcba', 1.0], ['abcaca', 1.0], ['cbcabac', 1.0], ['acababacbc', 1.0], ['cababcabac', 1.0], ['abcbcba', 1.0], ['acbacbabca', 1.0], ['abacacabca', 1.0], ['acabacaca', 1.0], ['bcacbabac', 1.0], ['cbacbaba', 1.0], ['cbcbcaca', 1.0], ['acbcbacba', 1.0], ['cbcacabac', 1.0], ['cacba', 1.0], ['bacbc', 1.0], ['cbcbac', 1.0], ['bcab', 1.0], ['bacbabca', 1.0], ['abcababcb', 1.0], ['ababcbc', 1.0], ['bacbc', 1.0], ['bacabac', 1.0], ['cbcbacbc', 1.0], ['cbcabab', 1.0], ['acabab', 1.0], ['cacacbac', 1.0], ['bacacacb', 1.0], ['bacbcacab', 1.0], ['acacabca', 1.0], ['cbcacaca', 1.0], ['bacacbacab', 1.0], ['caba', 1.0], ['cbacbcbaca', 1.0], ['abcabc', 1.0], ['bcbacbab', 1.0], ['bcbabacaba', 1.0], ['acbc', 1.0], ['acacbcba', 1.0], ['abac', 1.0], ['acabacba', 1.0], ['bcbcbc', 1.0], ['acabcbab', 1.0], ['cabcb', 1.0], ['cbcacacab', 1.0], ['babcbcbabc', 1.0], ['bcab', 1.0], ['cacabab', 1.0], ['abacbc', 1.0], ['cacabab', 1.0], ['cacba', 1.0], ['cababcba', 1.0], ['cabcbabcb', 1.0], ['cbcbc', 1.0], ['cacac', 1.0], ['acbacaca', 1.0], ['baba', 1.0], ['cbcac', 1.0], ['acbacababa', 1.0], ['cbcbcbc', 1.0], ['acbcbacb', 1.0], ['bcabcbcab', 1.0], ['babab', 1.0], ['bcbcba', 1.0], ['acbcaca', 1.0], ['acbabcbca', 1.0], ['acbabacb', 1.0], ['cbabcba', 1.0], ['cabca', 1.0], ['acab', 1.0], ['cbcbca', 1.0], ['bcbcbc', 1.0], ['cacbcabcac', 1.0], ['bcba', 1.0], ['bacbababac', 1.0], ['bacabac', 1.0], ['bcacacba', 1.0], ['acacbac', 1.0], ['bcbca', 1.0], ['bcbcbcacac', 1.0], ['cbcbcb', 1.0], ['cbacac', 1.0], ['abacbab', 1.0], ['cbaba', 1.0], ['abcacbcb', 1.0], ['bcbcba', 1.0], ['cabcacbcbc', 1.0], ['abcab', 1.0], ['bcacb', 1.0], ['abca', 1.0], ['bacabcab', 1.0], ['cbcabcab', 1.0], ['bcbcba', 1.0], ['babab', 1.0], ['bcabcbacb', 1.0], ['bacabca', 1.0], ['acacbcbc', 1.0], ['bacabab', 1.0], ['cbca', 1.0], ['cbab', 1.0], ['cabacbcaca', 1.0], ['bcabcbab', 1.0], ['cacab', 1.0], ['bacabacbcb', 1.0], ['babcba', 1.0], ['bacbcabac', 1.0], ['acabcac', 1.0], ['cabcaba', 1.0], ['acbcababca', 1.0], ['ababcbcbc', 1.0], ['bacacbcaba', 1.0], ['cbacacb', 1.0], ['cbcaca', 1.0], ['abca', 1.0], ['abcbacbc', 1.0], ['cacbc', 1.0], ['cababac', 1.0], ['acacbab', 1.0], ['cacbcbabc', 1.0], ['ababcacaca', 1.0], ['ababacbac', 1.0], ['acbcbacaca', 1.0], ['cbac', 1.0], ['cbcbaba', 1.0], ['acacbabcbc', 1.0], ['acabacb', 1.0], ['cacbacacba', 1.0], ['cacabca', 1.0], ['ababc', 1.0], ['bacacbcaca', 1.0], ['cbabca', 1.0], ['bababac', 1.0], ['acaba', 1.0], ['abcbacbcac', 1.0], ['cbcacba', 1.0], ['ababacab', 1.0], ['cbabcac', 1.0], ['abcaba', 1.0], ['cbcb', 1.0], ['cbacbcacb', 1.0], ['babcabacb', 1.0], ['babcac', 1.0], ['cababcaba', 1.0], ['acacb', 1.0], ['babababc', 1.0], ['cbca', 1.0], ['cacbca', 1.0], ['cacb', 1.0], ['babacabcb', 1.0], ['babcbca', 1.0], ['cabcababcb', 1.0], ['cbca', 1.0], ['abcacba', 1.0], ['bcbcab', 1.0], ['bcbababcb', 1.0], ['ababcacb', 1.0], ['ababcb', 1.0], ['cbacabacb', 1.0], ['acaca', 1.0], ['cabacac', 1.0], ['bcbacbc', 1.0], ['bacababacb', 1.0], ['acac', 1.0], ['abacabcbc', 1.0], ['bcbcba', 1.0], ['cbca', 1.0], ['bcabacbab', 1.0], ['cbcbc', 1.0], ['acac', 1.0], ['cabacabcba', 1.0], ['cbacbcacba', 1.0], ['babcacbc', 1.0], ['cabcbabca', 1.0], ['acacba', 1.0], ['abcb', 1.0], ['abcbacabc', 1.0], ['bcac', 1.0], ['babab', 1.0], ['cababcba', 1.0], ['cbcbacabca', 1.0], ['cbcab', 1.0], ['abacbcbac', 1.0], ['bacbabcac', 1.0], ['cbabacbacb', 1.0], ['cacacba', 1.0], ['cbcbabac', 1.0], ['cbabcabac', 1.0], ['bcbcacab', 1.0], ['acababcbca', 1.0], ['bcaba', 1.0], ['acacbcabc', 1.0], ['bcbcbabc', 1.0], ['abaca', 1.0], ['bacbc', 1.0], ['acacbacac', 1.0], ['acbc', 1.0], ['caca', 1.0], ['babacaba', 1.0], ['babcbcaca', 1.0], ['cbcab', 1.0], ['cabac', 1.0], ['ababc', 1.0], ['acbacbac', 1.0], ['cacacbcbca', 1.0], ['abacbc', 1.0], ['acbcbc', 1.0], ['bcabac', 1.0], ['cacbabca', 1.0], ['cbcacaca', 1.0], ['bacbca', 1.0], ['caba', 1.0], ['caba', 1.0], ['ababac', 1.0], ['acbababa', 1.0], ['acbac', 1.0], ['acaca', 1.0], ['cacacacaca', 1.0], ['bcacba', 1.0], ['bcbcabac', 1.0], ['abcbcbcba', 1.0], ['acabab', 1.0], ['bacbcabc', 1.0], ['acabcaca', 1.0], ['cacbcba', 1.0], ['cacacb', 1.0], ['acabcabcbc', 1.0], ['abcabcbcac', 1.0], ['bcbac', 1.0], ['acbcab', 1.0], ['acbabca', 1.0], ['abacbab', 1.0], ['cbacacb', 1.0], ['cbcb', 1.0], ['bacacacac', 1.0], ['abcb', 1.0], ['babcbaca', 1.0], ['abcbabacab', 1.0], ['cabaca', 1.0], ['bcbcbabc', 1.0], ['cbcbca', 1.0], ['acbcba', 1.0], ['cacab', 1.0], ['abacbcba', 1.0], ['bcaba', 1.0], ['acacbabca', 1.0], ['cacabcbc', 1.0], ['cacac', 1.0], ['bcabcaba', 1.0], ['babacacac', 1.0], ['cbcacbac', 1.0], ['cbcbabcaba', 1.0], ['acacb', 1.0], ['abacbcba', 1.0], ['acbcbcb', 1.0], ['cbabab', 1.0], ['abcbcbacb', 1.0], ['cacb', 1.0], ['cbcacb', 1.0], ['acac', 1.0], ['bacbcb', 1.0], ['bacab', 1.0], ['cbab', 1.0], ['bacab', 1.0], ['acbac', 1.0], ['cbacbcba', 1.0], ['abaca', 1.0], ['cbcbababab', 1.0], ['acbc', 1.0], ['acaca', 1.0], ['cabababc', 1.0], ['bcabcabab', 1.0], ['caba', 1.0], ['bababcbac', 1.0], ['abcbab', 1.0], ['cacbacbc', 1.0], ['babacabcb', 1.0], ['bcacab', 1.0], ['bacbcb', 1.0], ['abacb', 1.0], ['abacb', 1.0], ['ababcac', 1.0], ['babcbab', 1.0], ['baca', 1.0], ['babcababca', 1.0], ['cbacac', 1.0], ['cbcbc', 1.0], ['acbacba', 1.0], ['cbcacbacb', 1.0], ['acabababab', 1.0], ['abcaba', 1.0], ['cabcbca', 1.0], ['bacbcaba', 1.0], ['babcb', 1.0], ['bcbcbcbc', 1.0], ['cacbcbcb', 1.0], ['acbacab', 1.0], ['babcbcb', 1.0], ['cbacb', 1.0], ['bcbababcbc', 1.0], ['cacacbc', 1.0], ['abacbacaca', 1.0], ['cbcbababca', 1.0], ['abca', 1.0], ['bcbc', 1.0], ['cacbca', 1.0], ['cacbcbcabc', 1.0], ['bacab', 1.0], ['cabab', 1.0], ['baca', 1.0], ['cbcacabca', 1.0], ['bcabc', 1.0], ['bcacbacbc', 1.0], ['babcba', 1.0], ['baca', 1.0], ['cabcbabaca', 1.0], ['cabacbc', 1.0], ['cacabcb', 1.0], ['acbcb', 1.0], ['bacbab', 1.0], ['bcbabcbc', 1.0], ['bcbabcbcb', 1.0], ['abcbc', 1.0], ['cabcbcba', 1.0], ['cbab', 1.0], ['acbcaba', 1.0], ['cbacabc', 1.0], ['babacaca', 1.0], ['bcabcaba', 1.0], ['bcabcac', 1.0], ['abcabab', 1.0], ['cababcbcbc', 1.0], ['abab', 1.0], ['cbacb', 1.0], ['abacbacbca', 1.0], ['cacbac', 1.0], ['cbabcacac', 1.0], ['acbac', 1.0], ['caba', 1.0], ['bcbacbcac', 1.0], ['acbacbac', 1.0], ['bcabcacaca', 1.0], ['ababab', 1.0], ['bacbca', 1.0], ['ababcacb', 1.0], ['babab', 1.0], ['cbcbcac', 1.0], ['cbaca', 1.0], ['acbca', 1.0], ['cabacbabca', 1.0], ['ababcbcac', 1.0], ['cacbcbcacb', 1.0], ['cacabac', 1.0], ['bcabacabc', 1.0], ['bcbabac', 1.0], ['babcb', 1.0], ['babcbacacb', 1.0], ['abca', 1.0], ['cacac', 1.0], ['cbac', 1.0], ['cababa', 1.0], ['ababac', 1.0], ['cbacababac', 1.0], ['bcbc', 1.0], ['abacac', 1.0], ['cbabcbcb', 1.0], ['bcabc', 1.0], ['bacb', 1.0], ['acacab', 1.0], ['bcacba', 1.0], ['bacaca', 1.0], ['cacb', 1.0], ['acacabcb', 1.0], ['abab', 1.0], ['abcbcaca', 1.0], ['cacbcababa', 1.0], ['abcacb', 1.0], ['cacacb', 1.0], ['cabcbac', 1.0], ['bacb', 1.0], ['cabcbacaba', 1.0], ['aaabacb', 0.0], ['acbacc', 0.0], ['baacac', 0.0], ['abccacbba', 0.0], ['accaccbbac', 0.0], ['bccbbbbac', 0.0], ['caacc', 0.0], ['aaacabba', 0.0], ['cacacccb', 0.0], ['aaccccaa', 0.0], ['aacaabcc', 0.0], ['bcaacccbc', 0.0], ['cbaabb', 0.0], ['cabcba', 1.0], ['bccccbc', 0.0], ['abbbbab', 0.0], ['babba', 0.0], ['cbcb', 1.0], ['ccbab', 0.0], ['aaccaaaaba', 0.0], ['baaabbbbb', 0.0], ['abbcba', 0.0], ['acbbab', 0.0], ['cacbcbb', 0.0], ['bcaccacbca', 0.0], ['cbcbbcaab', 0.0], ['ccbcbbcbab', 0.0], ['bcbcabb', 0.0], ['ccacccacb', 0.0], ['bbca', 0.0], ['cbccc', 0.0], ['ccaacacb', 0.0], ['baaabaacab', 0.0], ['bbcccaac', 0.0], ['ccaacbab', 0.0], ['bcccac', 0.0], ['cbbb', 0.0], ['acccca', 0.0], ['cbbcbaccb', 0.0], ['aaacccba', 0.0], ['baabbca', 0.0], ['bbcba', 0.0], ['babbaabb', 0.0], ['babc', 1.0], ['bbbc', 0.0], ['aacbaccab', 0.0], ['baacacc', 0.0], ['caca', 1.0], ['ccba', 0.0], ['aabcabbcab', 0.0], ['baababab', 0.0], ['bacaabaa', 0.0], ['bcacbbb', 0.0], ['accbbbb', 0.0], ['babacb', 1.0], ['baaacacab', 0.0], ['aabbcbccbb', 0.0], ['ccbb', 0.0], ['bcaaba', 0.0], ['aabbcac', 0.0], ['bbababca', 0.0], ['ccbabb', 0.0], ['baaaabcb', 0.0], ['ccccccbbc', 0.0], ['abca', 1.0], ['baab', 0.0], ['bbbbcbbac', 0.0], ['acaacbb', 0.0], ['bbacbcab', 0.0], ['cabbcccbca', 0.0], ['aabc', 0.0], ['cacb', 1.0], ['bccabaacc', 0.0], ['caacabacbb', 0.0], ['cbcc', 0.0], ['bacbaa', 0.0], ['abcaaa', 0.0], ['acac', 1.0], ['bbccbbb', 0.0], ['aacaa', 0.0], ['aababa', 0.0], ['acabbb', 0.0], ['bccbbb', 0.0], ['bcbb', 0.0], ['acac', 1.0], ['aacccaa', 0.0], ['baba', 1.0], ['ababcaa', 0.0], ['aabbabc', 0.0], ['acbbc', 0.0], ['aaacbca', 0.0], ['abccbbccb', 0.0], ['bccbc', 0.0], ['babb', 0.0], ['accabbcac', 0.0], ['ccbabccc', 0.0], ['accababaa', 0.0], ['acbbccba', 0.0], ['bacbc', 1.0], ['abcaa', 0.0], ['cccca', 0.0], ['bcbbcbccbb', 0.0], ['baabaccba', 0.0], ['aaacacabb', 0.0], ['acbacbcabb', 0.0], ['bcabcba', 1.0], ['bbbbbaabbc', 0.0], ['acaaccacb', 0.0], ['bcaa', 0.0], ['aaccaccba', 0.0], ['cbaaa', 0.0], ['cbbcccbcbc', 0.0], ['acccca', 0.0], ['aabaabaaab', 0.0], ['bbaacb', 0.0], ['cbbbbbaaa', 0.0], ['bacbbca', 0.0], ['cbbbaa', 0.0], ['aacbc', 0.0], ['ccbcaacc', 0.0], ['cbacabb', 0.0], ['bbccac', 0.0], ['cbcb', 1.0], ['bcbaabc', 0.0], ['caaaaaab', 0.0], ['abbabaccaa', 0.0], ['abacabcca', 0.0], ['caacbcbab', 0.0], ['aabc', 0.0], ['ccabcbccc', 0.0], ['acaab', 0.0], ['baacccaca', 0.0], ['cbcacaabcb', 0.0], ['baaccabbb', 0.0], ['cbacabba', 0.0], ['cbbba', 0.0], ['acbbbaaaab', 0.0], ['cccccba', 0.0], ['baaabb', 0.0], ['cacca', 0.0], ['ccccbbbcac', 0.0], ['babcabca', 1.0], ['cccb', 0.0], ['acbac', 1.0], ['bcbac', 1.0], ['bcbcac', 1.0], ['bbcbcbccc', 0.0], ['bbcc', 0.0], ['bcaac', 0.0], ['cbcc', 0.0], ['baaaaaacac', 0.0], ['babaaacac', 0.0], ['cabcaaccca', 0.0], ['bcbacbc', 1.0], ['bbbbabaab', 0.0], ['bababba', 0.0], ['ababa', 1.0], ['baabbccaa', 0.0], ['acacabbbc', 0.0], ['cabacaaaaa', 0.0], ['abaccaaca', 0.0], ['aaabcbca', 0.0], ['baabccb', 0.0], ['baaaacaaba', 0.0], ['bcabc', 1.0], ['cbcccababc', 0.0], ['cabcab', 1.0], ['cccbacc', 0.0], ['accccc', 0.0], ['ccabcba', 0.0], ['cabacbbca', 0.0], ['cabb', 0.0], ['acabaaac', 0.0], ['cbaacac', 0.0], ['bcccabbbca', 0.0], ['acbacb', 1.0], ['aaacacacbb', 0.0], ['abaccbcac', 0.0], ['aacbbccca', 0.0], ['bbaaaa', 0.0], ['aacccbcbab', 0.0], ['bbaabbac', 0.0], ['bbabbaa', 0.0], ['ccbb', 0.0], ['abbbbca', 0.0], ['bcaabab', 0.0], ['cbaac', 0.0], ['accc', 0.0], ['aaacaaba', 0.0], ['acbbccb', 0.0], ['abcc', 0.0], ['abaa', 0.0], ['aabcc', 0.0], ['bbab', 0.0], ['bbcbbcb', 0.0], ['bbabcbb', 0.0], ['bbbb', 0.0], ['cbaacccaa', 0.0], ['abbbbbcbb', 0.0], ['bbabbcab', 0.0], ['babcaac', 0.0], ['ccacababc', 0.0], ['ccbcacb', 0.0], ['cbaa', 0.0], ['abbacac', 0.0], ['cbbca', 0.0], ['bacbba', 0.0], ['ccba', 0.0], ['caababcbc', 0.0], ['babb', 0.0], ['acbccbc', 0.0], ['ccbac', 0.0], ['ccbcbcabca', 0.0], ['aacbabacac', 0.0], ['bbcabca', 0.0], ['cbba', 0.0], ['bacaccba', 0.0], ['ccac', 0.0], ['abacbb', 0.0], ['ccba', 0.0], ['bacaaba', 0.0], ['bbcb', 0.0], ['bcabcbcaac', 0.0], ['bbcbabc', 0.0], ['babccab', 0.0], ['acacaccbc', 0.0], ['cbcaaba', 0.0], ['ccaba', 0.0], ['cbacbabb', 0.0], ['bcaab', 0.0], ['accbacbcbc', 0.0], ['bbac', 0.0], ['bbabcb', 0.0], ['bcabccaba', 0.0], ['cbcbbca', 0.0], ['aacba', 0.0], ['bacabbcba', 0.0], ['aabcbac', 0.0], ['aabc', 0.0], ['abccbcba', 0.0], ['accb', 0.0], ['babaccbaba', 0.0], ['caabc', 0.0], ['bcca', 0.0], ['cbaac', 0.0], ['cabb', 0.0], ['bccb', 0.0], ['abcbb', 0.0], ['abbcbca', 0.0], ['ccac', 0.0], ['bcbcbbcbac', 0.0], ['accb', 0.0], ['bcabcbccbc', 0.0], ['caab', 0.0], ['caababa', 0.0], ['abbab', 0.0], ['accbcac', 0.0], ['bbcab', 0.0], ['cababcacc', 0.0], ['cbabbcb', 0.0], ['bcaab', 0.0], ['cacbbac', 0.0], ['aaca', 0.0], ['acbbcb', 0.0], ['babcacaac', 0.0], ['abbca', 0.0], ['abaccaba', 0.0], ['cbcbb', 0.0], ['abbac', 0.0], ['acabcca', 0.0], ['abcacabb', 0.0], ['baabababa', 0.0], ['bbcabcbacb', 0.0], ['bacaac', 0.0], ['acca', 0.0], ['acbbaca', 0.0], ['ababaacac', 0.0], ['abbcbacab', 0.0], ['cbababbcab', 0.0], ['ccacacb', 0.0], ['bababaccac', 0.0], ['bbcacacb', 0.0], ['caccb', 0.0], ['cbcbbab', 0.0], ['baabcabc', 0.0], ['baababca', 0.0], ['caccbc', 0.0], ['abbcb', 0.0], ['acbccacbca', 0.0], ['bacbcc', 0.0], ['bcabaac', 0.0], ['cacaccab', 0.0], ['bcabaacb', 0.0], ['acbbc', 0.0], ['bbaba', 0.0], ['abbc', 0.0], ['cbcabbcb', 0.0], ['babccb', 0.0], ['ccbcabaca', 0.0], ['abcbccab', 0.0], ['cacca', 0.0], ['ccaba', 0.0], ['abacabaab', 0.0], ['cabbc', 0.0], ['cbaacacac', 0.0], ['babacc', 0.0], ['cbccbcacb', 0.0], ['aabcbca', 0.0], ['cbbcaba', 0.0], ['bccabab', 0.0], ['acaccb', 0.0], ['abaccba', 0.0], ['aacaba', 0.0], ['cabcaccab', 0.0], ['acbaacba', 0.0], ['aacba', 0.0], ['aabc', 0.0], ['cbcabacbcc', 0.0], ['abcacca', 0.0], ['aaba', 0.0], ['cacaabac', 0.0], ['bcacaa', 0.0], ['acaacacb', 0.0], ['bacaaba', 0.0], ['acaacbc', 0.0], ['bcacaab', 0.0], ['acbbacbc', 0.0], ['abbcbc', 0.0], ['bcaa', 0.0], ['cbbabcb', 0.0], ['abacaa', 0.0], ['abcacbbcb', 0.0], ['baccab', 0.0], ['bacbcacaa', 0.0], ['abcca', 0.0], ['acacacbcbb', 0.0], ['cacc', 0.0], ['cbccabcb', 0.0], ['babbcaba', 0.0], ['abacaacb', 0.0], ['aacab', 0.0], ['caacabcb', 0.0], ['cbba', 0.0], ['abaabcacb', 0.0], ['aaca', 0.0], ['cabcbbca', 0.0], ['aaba', 0.0], ['bcbcabcaa', 0.0], ['acabbcacbc', 0.0], ['bbac', 0.0], ['cabcabcc', 0.0], ['bcacabb', 0.0], ['cbacaabacb', 0.0], ['cacbccabc', 0.0], ['ccacab', 0.0], ['bccbcb', 0.0], ['bbcba', 0.0], ['bacbbcac', 0.0], ['ababbcac', 0.0], ['cbbabcbab', 0.0], ['aaba', 0.0], ['caac', 0.0], ['bccababcac', 0.0], ['abacacbaab', 0.0], ['ccbcbabc', 0.0], ['aaba', 0.0], ['bbcaba', 0.0], ['caccbc', 0.0], ['cbcbcc', 0.0], ['babbabacab', 0.0], ['bbcabc', 0.0], ['cacc', 0.0], ['bbcac', 0.0], ['accacbcac', 0.0], ['cbcaabac', 0.0], ['abaccacaca', 0.0], ['bacaccabab', 0.0], ['bbabcabc', 0.0], ['abacabaaba', 0.0], ['babbcbcaba', 0.0], ['cbcaac', 0.0], ['abcbcabbca', 0.0], ['accacbc', 0.0], ['cacbbc', 0.0], ['ccbabc', 0.0], ['cbcacc', 0.0], ['ababccbaca', 0.0], ['ccbacbacac', 0.0], ['ccaba', 0.0], ['bacca', 0.0], ['cbcabcc', 0.0], ['accaca', 0.0], ['bcbcabbca', 0.0], ['abbc', 0.0], ['cbaabc', 0.0], ['abccacba', 0.0], ['abccacbc', 0.0], ['bacabacbba', 0.0], ['acacbcc', 0.0], ['cabccaca', 0.0], ['babcaabacb', 0.0], ['ccab', 0.0], ['bcbcbacca', 0.0], ['babcabcaa', 0.0], ['acbacabba', 0.0], ['bccb', 0.0], ['cabba', 0.0], ['cacaa', 0.0], ['ccabcbabac', 0.0], ['cabcc', 0.0], ['cacbcbbac', 0.0], ['bcca', 0.0], ['acacaa', 0.0], ['abcabba', 0.0], ['bbabcba', 0.0], ['abacababaa', 0.0], ['cbabcbb', 0.0], ['aacabcaca', 0.0], ['caccb', 0.0], ['ccbc', 0.0], ['bbacb', 0.0], ['bbca', 0.0], ['bcbcca', 0.0], ['cbacaac', 0.0], ['abba', 0.0], ['baabab', 0.0], ['bacabbac', 0.0], ['bbcba', 0.0], ['cbacbabbab', 0.0], ['ccacb', 0.0], ['abcbcbaac', 0.0], ['abcbbacbab', 0.0], ['bacacabccb', 0.0], ['bcbcc', 0.0], ['acaccbcba', 0.0], ['cacaccbaca', 0.0], ['cacaa', 0.0], ['bacaabab', 0.0], ['bbababa', 0.0], ['bccab', 0.0], ['abccbcab', 0.0], ['bcbcabaa', 0.0], ['aacabca', 0.0], ['cabaccbc', 0.0], ['bbcbcbcaca', 0.0], ['babcaacb', 0.0], ['abcca', 0.0], ['cabbca', 0.0], ['caabacab', 0.0], ['ccababa', 0.0], ['cbccacbac', 0.0], ['abaa', 0.0], ['bcabaabc', 0.0], ['cabbcbc', 0.0], ['baccb', 0.0], ['babaabab', 0.0], ['bcacaa', 0.0], ['bccbabc', 0.0], ['cacacaaca', 0.0], ['abcc', 0.0], ['cbcabcbb', 0.0], ['bccabcacb', 0.0], ['bcacbaccba', 0.0], ['abababcca', 0.0], ['acbcbaa', 0.0], ['abccab', 0.0], ['cbaccb', 0.0], ['abccab', 0.0], ['cbacbabbca', 0.0], ['acbaaba', 0.0], ['cbbcb', 0.0], ['bcbcc', 0.0], ['cbbcba', 0.0], ['cbabbcba', 0.0], ['aacbab', 0.0], ['ababaabcac', 0.0], ['bacacaacba', 0.0], ['bbca', 0.0], ['ababaa', 0.0], ['cabb', 0.0], ['caccabcaca', 0.0], ['cbabcaaca', 0.0], ['bacaacacb', 0.0], ['acaabc', 0.0], ['abbacbcb', 0.0], ['bacaac', 0.0], ['bcaaca', 0.0], ['abbcb', 0.0], ['cabaacab', 0.0], ['bbcacacba', 0.0], ['bbacabab', 0.0], ['acbb', 0.0], ['aacbacbcba', 0.0], ['caacabcac', 0.0], ['caacbaca', 0.0], ['cbabccbca', 0.0], ['cbacabbabc', 0.0], ['bcbacbcaa', 0.0], ['baccbababa', 0.0], ['bcacbccacb', 0.0], ['ccab', 0.0], ['baab', 0.0], ['acaaba', 0.0], ['bcbccb', 0.0], ['bbcbac', 0.0], ['cbaab', 0.0], ['cacca', 0.0], ['accbcb', 0.0], ['bccacbca', 0.0], ['ababba', 0.0], ['cacaa', 0.0], ['bcbbcb', 0.0], ['abccab', 0.0], ['cbaabcac', 0.0], ['aacbaca', 0.0], ['cbbacabac', 0.0], ['cbabbcbcbc', 0.0], ['abacbccaca', 0.0], ['aababcac', 0.0], ['cabcbabbcb', 0.0], ['abcbaabcb', 0.0], ['babbaba', 0.0], ['cabacbccbc', 0.0], ['ccaba', 0.0], ['abaabca', 0.0], ['cbaac', 0.0], ['bacbcbbac', 0.0], ['cbcca', 0.0], ['abcaabc', 0.0], ['bbcbacb', 0.0], ['abcc', 0.0], ['cabccacb', 0.0], ['acbb', 0.0], ['cacbb', 0.0], ['bcbabaa', 0.0], ['cbacabcaab', 0.0], ['cbabbab', 0.0], ['ccaba', 0.0], ['cabcbcabb', 0.0], ['abcbabba', 0.0], ['baaca', 0.0], ['baccbcbc', 0.0], ['bcacbbca', 0.0], ['baacab', 0.0], ['bcaacbcab', 0.0], ['cbcabbcab', 0.0], ['abcc', 0.0], ['bcbcabbcb', 0.0], ['aacababc', 0.0], ['ccabcacbca', 0.0], ['acbb', 0.0], ['caacacba', 0.0], ['acabbcb', 0.0], ['cbacbcaab', 0.0], ['bcaccbcb', 0.0], ['bbacb', 0.0], ['babacacacc', 0.0], ['bcaab', 0.0], ['bbabca', 0.0], ['baabac', 0.0], ['aabcabcab', 0.0], ['cbabaa', 0.0], ['abcbbcab', 0.0], ['cbccbc', 0.0], ['abcbabb', 0.0], ['caaba', 0.0], ['abbababa', 0.0], ['cbbcba', 0.0], ['cbaa', 0.0], ['bcababb', 0.0], ['cabaacacb', 0.0], ['bcbb', 0.0], ['abcbbab', 0.0], ['abcaccba', 0.0], ['ababacbb', 0.0], ['acbba', 0.0], ['babb', 0.0], ['cbbc', 0.0], ['bcbcca', 0.0], ['acbbcaba', 0.0], ['abbcbcba', 0.0], ['bacacaaca', 0.0], ['ababaaca', 0.0], ['ccacabc', 0.0], ['aaca', 0.0], ['ababbc', 0.0], ['acbabcc', 0.0], ['cabccab', 0.0], ['bcabba', 0.0], ['bacbaaba', 0.0], ['acbbca', 0.0], ['abcacbbcab', 0.0], ['cabbcbcab', 0.0], ['abbab', 0.0], ['bcababaa', 0.0], ['bbabcba', 0.0], ['cbaacb', 0.0], ['cbabbacaba', 0.0], ['cbaccba', 0.0], ['cabbac', 0.0], ['acabcbcaab', 0.0], ['bcaab', 0.0], ['cbaababa', 0.0], ['ccbcbacbca', 0.0], ['bacbcaab', 0.0], ['bbacac', 0.0], ['bccaca', 0.0], ['bababb', 0.0], ['bcbabb', 0.0], ['bcacbbcbc', 0.0], ['aaca', 0.0], ['bbca', 0.0], ['acacaa', 0.0], ['ccacb', 0.0], ['aacb', 0.0], ['baccbacb', 0.0], ['bcbacca', 0.0], ['bacc', 0.0], ['bcbbac', 0.0], ['aabc', 0.0], ['baacaca', 0.0], ['abccbaba', 0.0], ['cbabcabaac', 0.0], ['ccbcaca', 0.0], ['baabcacba', 0.0], ['cacbbac', 0.0], ['bcabccbacb', 0.0], ['cbbcb', 0.0], ['bbcacbc', 0.0], ['ababaa', 0.0], ['aabac', 0.0], ['cbbab', 0.0], ['bbcbc', 0.0], ['abbaca', 0.0], ['cbccacabc', 0.0], ['bacaab', 0.0], ['abccbcab', 0.0], ['aabcacacb', 0.0], ['cabcbb', 0.0], ['ccab', 0.0], ['bacbcbbabc', 0.0], ['abacacacc', 0.0], ['aabc', 0.0], ['acaabc', 0.0], ['ccbab', 0.0], ['cbcc', 0.0], ['cbcbcbbab', 0.0], ['cacbbaca', 0.0], ['cbbabcbc', 0.0], ['cbaccba', 0.0], ['cbcc', 0.0], ['abcaccb', 0.0], ['acaac', 0.0], ['cacbacbba', 0.0], ['cabcabcbbc', 0.0], ['acabacbba', 0.0], ['acbba', 0.0], ['bcaac', 0.0], ['ccbacba', 0.0], ['cabacaacac', 0.0], ['accabcabc', 0.0], ['bccbcaca', 0.0], ['ccbcb', 0.0], ['bcaacababa', 0.0], ['bbcacacba', 0.0], ['bccabcba', 0.0], ['baccacbcba', 0.0], ['abcaab', 0.0], ['bacbbac', 0.0], ['ccabcb', 0.0], ['cacbaabca', 0.0], ['abcbb', 0.0], ['cbcaa', 0.0], ['acabcaccbc', 0.0], ['cbbaca', 0.0], ['cbbab', 0.0], ['bcca', 0.0], ['babaccb', 0.0], ['ccababc', 0.0], ['bacaaca', 0.0], ['cacc', 0.0], ['acacabb', 0.0], ['bacbb', 0.0], ['acbcabba', 0.0], ['cbaccacbc', 0.0], ['cacabbacb', 0.0], ['bcbbcacbc', 0.0], ['cabaab', 0.0], ['ccbc', 0.0], ['aacbcbcbc', 0.0], ['cbbcab', 0.0], ['bccac', 0.0], ['bcbaabac', 0.0], ['acbbabc', 0.0], ['abacbbcbcb', 0.0], ['abcbabb', 0.0], ['ababaacba', 0.0], ['abcaa', 0.0], ['cabbab', 0.0], ['cacbcbb', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of alphabets set according to the alphabet size, it is known before constructing a DFA\n",
    "DICTIONARY = ['a', 'b', 'c'] \n",
    "NUMBER_OF_CHARS = len(DICTIONARY)              # number of alphabets, |A|\n",
    "\n",
    "# getting the dataset of the language\n",
    "f = open('dataset/tl2_dataset.txt', 'r')  # get the dataset for a language\n",
    "dataset = []                              # array to hold the (w,ans) \n",
    "for line in f:\n",
    "    arr = line.split(' ')\n",
    "    ans = 1.0\n",
    "    if arr[1][0] == '0':\n",
    "        ans = 0.0\n",
    "    dataset.append([arr[0], ans])\n",
    "    \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-compatibility",
   "metadata": {},
   "source": [
    "### definition of all the function used in the RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bored-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function (y_true - y_predicted)^2\n",
    "def cost_function(exp, res):                   \n",
    "    return (res - exp) ** 2\n",
    "\n",
    "# derivative of loss function (ytrue - yloss)\n",
    "def cost_function_derivative(exp, res):       \n",
    "    return res - exp\n",
    "\n",
    "\n",
    "# Match function: takes the nn tensor, current state and the next character and performs the transition\n",
    "# ch is the next character and pos is the current position\n",
    "# Q x S > Q i.e next state = tensor x next character x current state\n",
    "def match(nn, ch, pos):                         \n",
    "    new_pos = np.zeros(NUMBER_OF_POSITIONS)         # create a new array for the next state\n",
    "    for k in range(NUMBER_OF_POSITIONS):            # for each value in the new state\n",
    "        for i in range (NUMBER_OF_CHARS):           # go through next character array\n",
    "            for j in range (NUMBER_OF_POSITIONS):   # go through current staste array\n",
    "                new_pos[k] += nn.tensor[k][i][j] * ch[i] * pos[j] # new state position is tensor x next character x current state\n",
    "    return new_pos                                  # return the new state\n",
    "\n",
    "# derivative of Match function wrt current state: takes the tensor, a gradient and next input character\n",
    "# derivative (2D array)= tensor x next character\n",
    "def match_derivative(nn, dz, ch):\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_POSITIONS]) # dim: |Q| x |Q|\n",
    "    for i in range(NUMBER_OF_POSITIONS):                   # for each column in derivative i.e values in current state [p1 p2 p3 ..]\n",
    "        for k in range(NUMBER_OF_POSITIONS):               # for each row in derivative i.e vlues in new state [h1 h2 h3 ..]\n",
    "            for j in range (NUMBER_OF_CHARS):              # loop through current character  \n",
    "                derivative[k][i] += nn.tensor[k][j][i] * ch[j] # derivative (2D array)= tensor x next character\n",
    "    return np.dot(dz, derivative)                          # dot product of previous gradient with new one\n",
    "\n",
    "# derivative of Match function wrt tensor: takes a gradient, next input character and current state \n",
    "# derivative(3D array) = next character x current state \n",
    "def match_derivative_tensor(dz, ch, pos):\n",
    "    sample_matrix = np.zeros([NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # create a two 2D array of next character and current state\n",
    "    for i in range(NUMBER_OF_CHARS):\n",
    "        for j in range(NUMBER_OF_POSITIONS):\n",
    "            sample_matrix[i][j] = ch[i] * pos[j]                     # fill the sample matrix vy multiplying next character and current state\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # new derivative as a 3D array\n",
    "    for k in range(NUMBER_OF_POSITIONS): # for each row in first dimension\n",
    "        derivative[k] = dz[k] * sample_matrix      # new derivative is old * sample matrix\n",
    "    return derivative                              # return derivative\n",
    "\n",
    "\n",
    "# normalize function for each value in h_k return h/sum(h)\n",
    "def normalize(h):                    \n",
    "    return h / np.sum(h)\n",
    "\n",
    "# derivative of normalize: takes a derivative, and input(h) to the output layer\n",
    "def normalize_derivative(dz, h):\n",
    "    derivative = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_POSITIONS])\n",
    "    sum = np.sum(h)\n",
    "    for i in range(NUMBER_OF_POSITIONS):      # for each value in the output layer vector \n",
    "        for j in range(NUMBER_OF_POSITIONS):  # for each value in the output layer vector\n",
    "            if(i == j):                       # if position of value wrt to which derivative is being taken is equal, \n",
    "                derivative[i][j] = (sum - h[i]) / (sum ** 2) # derivative is (sum - value)/sum^2\n",
    "            else:\n",
    "                derivative[i][j] = -h[i] / (sum ** 2)        # else -value/sum^2\n",
    "    return np.dot(dz, derivative)             # dot product of previous derivative with new one\n",
    "\n",
    "\n",
    "# applying adder function to output layer, takes the adder and the final state\n",
    "# final output neuron = adder x final state\n",
    "def lastsum(nn, x):\n",
    "    return np.dot(nn.adder, x) # final output neuron = adder x final state\n",
    "\n",
    "# derivative of adder wrt its argument i.e final output vector,to be used in tensor backpropagation \n",
    "# takes adder and a derivative\n",
    "def lastsum_derivative(nn, dz):\n",
    "    return np.dot(dz, nn.adder)      # dot product of adder values(k) and current derivative\n",
    "\n",
    "# derivative of adder wrt to k and output vector, to be used in adder backpropagation\n",
    "# takes adder, current deriviative and a output layer state\n",
    "def lastsum_derivative_adder(nn, dz, inp):\n",
    "    derivative = np.multiply(inp, nn.adder) # derivative is output layer state x adder \n",
    "    return np.dot(dz, derivative)\n",
    "\n",
    "# function to change a character to vector encodings\n",
    "def char_to_vector(ch):\n",
    "    index = DICTIONARY.index(ch)    # get the index of a character  from dictionary\n",
    "    vec = np.zeros(NUMBER_OF_CHARS) # get a 1D array of len(alphabet) x 1\n",
    "    vec[index] = 1.0                # change the index of character character to 1\n",
    "    return vec                      # return the vector\n",
    "\n",
    "# cut function which rounds up the value to 0 or 1, weights should be between 0 and 1\n",
    "def cut(x):\n",
    "    if (x > 1.0):        # if x is greater than 1, round it to 1\n",
    "        return 1.0\n",
    "    if (x < 0.0):\n",
    "        return 0.0       # if x is lesser than 0. round it to 0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-retailer",
   "metadata": {},
   "source": [
    "### RNN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "injured-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network object with two attributes and three methods\n",
    "# two attributes are tensor and adder which are alterable parameters of the network, tensor and adder\n",
    "# three methods are check, train, train_online, get_automaton, and run\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # dim: |Q| x |A| x |Q|, Q is number of states and |A| is alphabet size\n",
    "    tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS])  \n",
    "    adder = np.zeros(NUMBER_OF_POSITIONS)\n",
    "\n",
    "    def __init__(self): # init method that creates tensor and adder attribute and initializes it\n",
    "        \n",
    "        self.tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # dim: to x ch x fr\n",
    "        for fr in range(NUMBER_OF_POSITIONS):  # for each index in fr state\n",
    "            for ch in range(NUMBER_OF_CHARS):  # for each index in a character\n",
    "                z = np.random.rand(NUMBER_OF_POSITIONS) # get a random value intialized array of lenght |Q|\n",
    "                z = normalize(z)                        # normalize it \n",
    "                for to in range(NUMBER_OF_POSITIONS):   # for each index in to state\n",
    "                    self.tensor[to][ch][fr] = z[to]     # initialize the given index of tensor\n",
    "                    \n",
    "        self.adder = np.zeros(NUMBER_OF_POSITIONS)      # dim: |Q| x 1\n",
    "        for to in range(NUMBER_OF_POSITIONS):           # for each index in the adder array\n",
    "            # initialize each index with a value corresponding to its index, value = (index + 1)/|Q|\n",
    "            self.adder[to] = 1.0 * (to + 1) / (NUMBER_OF_POSITIONS) \n",
    "\n",
    "\n",
    "    def check(self, word): # method to check whether a word belongs to the language or not\n",
    "        curr_pos = START_POSITION                       # start from start position \n",
    "        for k in range(len(word)):                      # for each character in the word\n",
    "            curr_word = char_to_vector(word[k])         # convert it to vector form \n",
    "            curr_pos = match(self, curr_word, curr_pos) # make a transition \n",
    "            curr_pos = normalize(curr_pos)              # normalize the next state\n",
    "        output = lastsum(self, curr_pos)                # then apply an adder function to get a final output\n",
    "        return output\n",
    "\n",
    "    def train_online(self, dataset): # method to online train the network i.e weight are adjusted after every word is read\n",
    "        average_error = 1.0          # initializing and average error\n",
    "        epoch_number = 0             # epoch number\n",
    "        n = len(dataset)             # number of words in the datset\n",
    "        tests_size = int(PERCENT_OF_TESTS * n) # number of words allocated for testing the dataset\n",
    "        while (average_error > EPS):           # while the error is greater than specified epsilon  \n",
    "            random.shuffle(dataset)            # shuffle the datset for better training\n",
    "            cases_left = len(dataset)          # varaible to track the number of words lefts to train on\n",
    "            epoch_number += 1                  # increment the epch by 1\n",
    "            print ( 'Epoch #' + str(epoch_number))  # print epoch number\n",
    "            while(cases_left > tests_size):         # for a epoch train on the whole dataset \n",
    "                self.train(dataset[cases_left - 1][0], dataset[cases_left - 1][1]) # training is done from the last word\n",
    "                cases_left -= 1\n",
    "            average_error = 0.0                    \n",
    "            for i in range(cases_left):         # for each word in the dataset \n",
    "                average_error += cost_function(dataset[i][1], self.check(dataset[i][0])) # predict and calculate the error\n",
    "            average_error /= cases_left         # normalize the average error\n",
    "            print (\"Average error: \" + str(average_error)) # print the average error\n",
    "\n",
    "    def train(self, word, exp):    # method to train on a word (string), takes word and expected output(0 or 1)\n",
    "        cut_v = np.vectorize(cut)  # vectorize cut function as cut_v\n",
    "        word_length = len(word)    # length of the word to be trained on\n",
    "        \n",
    "        # array to track the states the input character leads to\n",
    "        positions = np.zeros([word_length + 1, NUMBER_OF_POSITIONS])     # dim: (word length + 1) x number of states\n",
    "        \n",
    "        # array to store the values after a character of a word has been consumed, i.e the raw values of the next state \n",
    "        # after transition t is applied on current state and next input character\n",
    "        before_normalize = np.zeros([word_length, NUMBER_OF_POSITIONS])  # dim: word length  x number of states\n",
    "        \n",
    "        # array to store the derivative of transition tensor \n",
    "        d_tensor = np.zeros([NUMBER_OF_POSITIONS, NUMBER_OF_CHARS, NUMBER_OF_POSITIONS]) # dim: |Q| x |A| x |Q|\n",
    "        \n",
    "        # array to store the derivative of adder \n",
    "        d_adder = np.zeros(NUMBER_OF_POSITIONS) # dim: |Q| x 1\n",
    "        positions[0] = START_POSITION           # 0th index of position is start position\n",
    "        \n",
    "        #  do a forward pass on the network i.e match and normalize until the end of the word, and then adder at the end\n",
    "        for k in range(word_length):             # for each character in word\n",
    "            curr_word = char_to_vector(word[k])  # change it to vector form                  \n",
    "            before_normalize[k] = match(self, curr_word, positions[k]) # apply match on it and save the raw output\n",
    "            positions[k + 1] = normalize(before_normalize[k])          # normalize the raw output and save the next state\n",
    "        answer = lastsum(self, positions[-1])    # compute the adder on final output from normalize, get the final value\n",
    "        \n",
    "        error = cost_function(exp, answer)       # computer the cost on obtain answer (y_true - y_predicted)^2 \n",
    "\n",
    "\n",
    "        gradient = cost_function_derivative(exp, answer)  # compute the gradient of the error\n",
    "        \n",
    "        # gradient calculation for adder backpropagation\n",
    "        d_adder += lastsum_derivative_adder(self, gradient, positions[-1]) # compute the gradient of adder wrt its weights\n",
    "        \n",
    "        # gradient calculation for tensor backpropagtion\n",
    "        gradient = lastsum_derivative(self, gradient)     # calculate the derivative of adder wrt final output vector\n",
    "        first_gradient = sum(abs(gradient)) * TenzToAdd   # sum the gradient and store double of it  \n",
    "        \n",
    "        # for tensor, gradient calculation needs to be done w times i.e length of word times.\n",
    "        for k in range(word_length - 1, -1, -1):  # for each character in the word, as we are backtracing       \n",
    "            curr_grad = sum(abs(gradient))        # get the sum of the gradient so far i.e ( error * adder gradient)\n",
    "            if (curr_grad < 0.001):               # if current gradient is very small  < 0.001\n",
    "                koef = 1.0                        # make the coeffiecient as 1\n",
    "            else:\n",
    "                koef = first_gradient / sum(abs(gradient)) # else change the coefficient accordingly\n",
    "            gradient *= koef                      # multiply the coeffiecient with gradient\n",
    "            curr_word = char_to_vector(word[k])   # change the character to vector form, \n",
    "            gradient = normalize_derivative(gradient, before_normalize[k])  # calculate gradient of normalize for the           \n",
    "            \n",
    "            d_tensor += match_derivative_tensor(gradient, curr_word, positions[k]) # calculate the gradient of match \n",
    "                                                  #   wrt its weights and add it to derivative of tensor\n",
    "            gradient = match_derivative(self, gradient, curr_word) # calculate the gradient of match \n",
    "                                                  # wrt its output state\n",
    "\n",
    "        d_tensor /= word_length                   # normalize the d_tensor\n",
    "        self.tensor = cut_v(self.tensor - NU * d_tensor)         # update the tensor weights\n",
    "        self.adder = cut_v(self.adder - NU * NU_ADDER * d_adder) # update the adder weights\n",
    "        return error\n",
    "\n",
    "    def get_automaton(self): # method to get the DFA, traverse through 3D array tensor\n",
    "        \n",
    "        for i in range(NUMBER_OF_POSITIONS): # for each index of a state, last index of 3D array, from state\n",
    "            for j in range(NUMBER_OF_CHARS): # for each index of a character, middle index of 3D array, character\n",
    "                max_ind = 0                  # set max_ind as 0 initially\n",
    "                for k in range(1, NUMBER_OF_POSITIONS): # for each index of a state, first index of 3D array, to state\n",
    "                    if (nn.tensor[k][j][i] > nn.tensor[max_ind][j][i]): # find the index with the maximum value \n",
    "                        max_ind = k                                     # set it to k\n",
    "                print(str(i) + \"--\" + str(DICTIONARY[j]) + '-->' + str(max_ind)) # then from(i) state upon consuming(j)\n",
    "                                                     # has maximum probability of going to to(k) state\n",
    "        \n",
    "        for k in range(NUMBER_OF_POSITIONS):  # for each index of a state in adder vector\n",
    "            if (nn.adder[k] > 0.5):           # if the value of the a particular index is greater than 0.5\n",
    "                print(str(k) + \" is terminal\") # then it has more than 50% chance to be terminal\n",
    "    \n",
    "    \n",
    "    def run(self, word): # method to run a word on the trained dfa and check whether the word gets accepted or rejected\n",
    "        output = nn.check(word)        # call check method\n",
    "        if (output > float(0.6)):      # if the probability is greater than 60 %\n",
    "            ans = 'Accepted'           # then accept the word\n",
    "        else: \n",
    "            ans = 'Rejected'           # or reject the word\n",
    "        return output, ans             # return both output and ans\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "compliant-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "Average error: 0.2596516268234478\n",
      "Epoch #2\n",
      "Average error: 0.03608710994119018\n",
      "Epoch #3\n",
      "Average error: 2.554988428934224e-09\n",
      "0--a-->1\n",
      "0--b-->3\n",
      "0--c-->4\n",
      "1--a-->2\n",
      "1--b-->3\n",
      "1--c-->4\n",
      "2--a-->2\n",
      "2--b-->2\n",
      "2--c-->2\n",
      "3--a-->1\n",
      "3--b-->2\n",
      "3--c-->4\n",
      "4--a-->1\n",
      "4--b-->3\n",
      "4--c-->2\n",
      "1 is terminal\n",
      "3 is terminal\n",
      "4 is terminal\n"
     ]
    }
   ],
   "source": [
    "# training the NN and getting the DFA\n",
    "\n",
    "nn = NeuralNetwork();\n",
    "nn.train_online(dataset)\n",
    "nn.get_automaton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "governing-telescope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected\n"
     ]
    }
   ],
   "source": [
    "output, ans = nn.run(\"abaaba\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "compliant-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.94517224e-02 1.00000000e+00 7.24567197e-05 1.00000000e+00\n",
      " 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(nn.adder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
